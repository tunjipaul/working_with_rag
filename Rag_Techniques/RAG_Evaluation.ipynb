{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation with RAGAS\n",
    "\n",
    "## The Problem\n",
    "How do you know if your RAG improvements actually work?\n",
    "\n",
    "**You need systematic measurement!**\n",
    "\n",
    "## The Solution\n",
    "**RAGAS:** Framework for evaluating RAG systems with metrics:\n",
    "- Context Recall\n",
    "- Context Precision\n",
    "- Faithfulness\n",
    "- Answer Relevancy\n",
    "\n",
    "**Difficulty:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAGAS?\n",
    "\n",
    "**RAGAS** (Retrieval-Augmented Generation Assessment) is an automated evaluation framework that uses LLMs to judge your RAG system's quality.\n",
    "\n",
    "**The key insight:** Instead of manually reviewing answers, RAGAS uses AI to evaluate:\n",
    "1. **Retrieval quality** - Did we find the right documents?\n",
    "2. **Generation quality** - Is the answer accurate and relevant?\n",
    "\n",
    "**Why it matters:**\n",
    "- Without metrics, you're flying blind\n",
    "- You need objective comparison between techniques\n",
    "\n",
    "Let's learn how to measure RAG performance scientifically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_openai import setup_openai_api, create_embeddings, create_llm, load_msme_data, create_vectorstore, get_baseline_prompt, load_existing_vectorstore\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_recall, faithfulness, answer_relevancy\n",
    "from ragas import EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print('[OK] Imports done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = setup_openai_api()\n",
    "embeddings = create_embeddings(api_key)\n",
    "llm = create_llm(api_key)\n",
    "vectorstore = load_existing_vectorstore(embeddings, 'msme_ragas', './chroma_raags')\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "prompt = get_baseline_prompt()\n",
    "rag_chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print('[OK] RAG system ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = setup_openai_api()\n",
    "embeddings = create_embeddings(api_key)\n",
    "llm = create_llm(api_key)\n",
    "docs, metas, ids = load_msme_data('msme.csv')\n",
    "vectorstore = create_vectorstore(docs, metas, ids, embeddings, 'msme_ragas', './chroma_raags')\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "prompt = get_baseline_prompt()\n",
    "rag_chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print('[OK] RAG system ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Ground Truth?**\n",
    "\n",
    "Ground truth = the \"correct\" or \"ideal\" answer you expect for each question.\n",
    "\n",
    "Think of it like a test answer key. RAGAS compares your RAG's actual answers against these reference answers to measure quality.\n",
    "\n",
    "**Note:** You don't need perfect ground truth - even approximate reference answers help RAGAS evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Evaluation Dataset\n",
    "Create test queries with ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    'What are the financing options for MSMEs in Nigeria?',\n",
    "    'How do I register a business in Nigeria?',\n",
    "    'What challenges do MSMEs face?'\n",
    "]\n",
    "\n",
    "eval_ground_truth = [\n",
    "    'MSMEs can access financing through Development Bank of Nigeria, Bank of Industry, microfinance banks, and government intervention funds.',\n",
    "    'Register with Corporate Affairs Commission (CAC), obtain Tax Identification Number (TIN), and get necessary licenses.',\n",
    "    'MSMEs face challenges including poor access to credit, weak infrastructure, discriminatory legislation, and lack of technical skills.'\n",
    "]\n",
    "\n",
    "print(f'[OK] Prepared {len(eval_questions)} test cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Answers and Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for question, reference in zip(eval_questions, eval_ground_truth):\n",
    "    # Retrieve documents\n",
    "    retrieved = retriever.invoke(question)\n",
    "    contexts = [doc.page_content for doc in retrieved]\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    dataset.append({\n",
    "        'user_input': question,\n",
    "        'retrieved_contexts': contexts,\n",
    "        'response': answer,\n",
    "        'reference': reference\n",
    "    })\n",
    "\n",
    "print(f'[OK] Generated {len(dataset)} evaluation samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's happening here:**\n",
    "\n",
    "For each test question, we're collecting 4 pieces of information:\n",
    "1. **user_input** - The question asked\n",
    "2. **retrieved_contexts** - Documents the retriever found\n",
    "3. **response** - Answer generated by the RAG system\n",
    "4. **reference** - Ground truth (expected answer)\n",
    "\n",
    "RAGAS needs all 4 to evaluate different aspects:\n",
    "- Context Recall uses: question + contexts + reference\n",
    "- Faithfulness uses: contexts + response\n",
    "- Answer Relevancy uses: question + response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why a separate evaluator LLM?**\n",
    "\n",
    "RAGAS uses an LLM to judge your RAG system's outputs. This \"judge\" LLM:\n",
    "- Compares retrieved contexts to reference answers\n",
    "- Checks if generated answers are faithful to context\n",
    "- Measures how relevant answers are to questions\n",
    "\n",
    "We wrap the same LLM we're testing with `LangchainLLMWrapper` so RAGAS can use it for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create RAGAS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "print('[OK] RAGAS dataset created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Evaluator LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "print('[OK] Evaluator LLM ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[context_recall, faithfulness, answer_relevancy],\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('RAGAS EVALUATION RESULTS')\n",
    "print('='*80)\n",
    "\n",
    "# Convert to pandas DataFrame for easier viewing\n",
    "result_df = result.to_pandas()\n",
    "print(result_df)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('AVERAGE SCORES')\n",
    "print('='*80)\n",
    "print(f'Context Recall: {result_df[\"context_recall\"].mean():.3f}')\n",
    "print(f'Faithfulness: {result_df[\"faithfulness\"].mean():.3f}')\n",
    "print(f'Answer Relevancy: {result_df[\"answer_relevancy\"].mean():.3f}')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Your Scores\n",
    "\n",
    "**All metrics range from 0 to 1 (higher is better)**\n",
    "\n",
    "### Quick Score Guide:\n",
    "- **0.9 - 1.0**: Excellent - Production ready\n",
    "- **0.7 - 0.9**: Good - Minor improvements needed\n",
    "- **0.5 - 0.7**: Fair - Significant issues to address\n",
    "- **Below 0.5**: Poor - Major problems with retrieval or generation\n",
    "\n",
    "### What to do if scores are low:\n",
    "- **Low Context Recall?** ‚Üí Improve retrieval (try hybrid search, increase k, better chunking)\n",
    "- **Low Faithfulness?** ‚Üí LLM is hallucinating ‚Üí Improve prompts, use RAG-specific models\n",
    "- **Low Answer Relevancy?** ‚Üí Answers off-topic ‚Üí Improve prompt instructions, better context filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to compare techniques:**\n",
    "\n",
    "1. Run the SAME test questions on each technique\n",
    "2. Record the three metrics for each\n",
    "3. Look for trade-offs:\n",
    "   - Some techniques improve recall but hurt faithfulness\n",
    "   - Others are slower but more accurate\n",
    "   \n",
    "**Pro tip:** No single technique wins everything. Choose based on your priorities:\n",
    "- Need accuracy? ‚Üí Prioritize Faithfulness\n",
    "- Missing documents? ‚Üí Prioritize Context Recall\n",
    "- Vague answers? ‚Üí Prioritize Answer Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Techniques\n",
    "Now you can objectively compare all techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comparison (you fill in actual scores)\n",
    "print('\\nTECHNIQUE COMPARISON')\n",
    "print('='*80)\n",
    "print(f'{\"Technique\":<30} {\"Recall\":<10} {\"Faithful\":<10} {\"Relevant\":<10}')\n",
    "print('-'*80)\n",
    "print(f'{\"Baseline\":<30} {0.85:<10.2f} {0.92:<10.2f} {0.88:<10.2f}')\n",
    "print(f'{\"BM25 Hybrid\":<30} {0.89:<10.2f} {0.91:<10.2f} {0.90:<10.2f}')\n",
    "print(f'{\"Contextual Compression\":<30} {0.83:<10.2f} {0.95:<10.2f} {0.91:<10.2f}')\n",
    "print(f'{\"HyDE\":<30} {0.88:<10.2f} {0.89:<10.2f} {0.92:<10.2f}')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use\n",
    "**Always!** Evaluation should be part of every RAG system.\n",
    "\n",
    "**Use RAGAS to:**\n",
    "- Compare techniques objectively\n",
    "- Track improvements over time\n",
    "- Identify weak points\n",
    "- Make data-driven decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Final Project:**\n",
    "1. Evaluate ALL 5 techniques on same test set\n",
    "2. Create comparison table\n",
    "3. Identify best technique for each metric\n",
    "4. Recommend optimal combination\n",
    "\n",
    "This is your capstone - show what you've learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your comprehensive evaluation code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed all 6 Advanced RAG Techniques!\n",
    "\n",
    "**What you learned:**\n",
    "- Multiple Query Retrieval\n",
    "- Contextual Compression\n",
    "- Semantic Chunking\n",
    "- Reranking\n",
    "- HyDE\n",
    "- RAGAS Evaluation\n",
    "\n",
    "**Next steps:**\n",
    "1. Build your own RAG system\n",
    "2. Combine techniques that work well together\n",
    "3. Deploy to production\n",
    "4. Keep learning!\n",
    "\n",
    "**Resources:**\n",
    "- Check README.md for decision guides\n",
    "- Explore https://github.com/NirDiamant/RAG_Techniques/tree/main for more\n",
    "\n",
    "**Happy building! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bao_env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
