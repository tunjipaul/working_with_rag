{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 4: Reranking\n",
    "\n",
    "## The Problem\n",
    "Initial retrieval isn't perfect. The top-5 docs might not be in optimal order.\n",
    "\n",
    "**Why?** Embedding models optimize for speed, not perfect ranking.\n",
    "\n",
    "## The Solution\n",
    "**Cross-Encoder Reranking:**\n",
    "1. Retrieve top-K docs (e.g., 20)\n",
    "2. Use powerful cross-encoder to rerank\n",
    "3. Take top-N after reranking (e.g., 5)\n",
    "\n",
    "Slower but more accurate!\n",
    "\n",
    "**Difficulty:** â­â­â­â­â˜†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ How Reranking Works: Two-Stage Retrieval\n",
    "\n",
    "**Stage 1: Fast Retrieval (Bi-Encoder)**\n",
    "- Retrieve MORE documents (e.g., top-10 or top-20)\n",
    "- Uses fast vector similarity (embeddings computed separately)\n",
    "- Goal: Cast a wide net - don't miss relevant docs\n",
    "\n",
    "**Stage 2: Accurate Reranking (Cross-Encoder)**\n",
    "- Take those candidates from Stage 1\n",
    "- Use powerful cross-encoder to score each query-doc pair\n",
    "- Keep only the BEST N (e.g., top-5)\n",
    "- Goal: Optimize ranking for maximum relevance\n",
    "\n",
    "### Example: Before vs After Reranking\n",
    "\n",
    "**Before Reranking (Vector Similarity):**\n",
    "```\n",
    "Query: \"MSME financing options\"\n",
    "\n",
    "Top-5 by cosine similarity:\n",
    "1. [SMEDAN overview] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.82\n",
    "2. [Financing options] â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.81  â† Should be #1!\n",
    "3. [Registration process] â”€â”€â”€â”€â”€â”€ 0.79\n",
    "4. [Tax benefits] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.77\n",
    "5. [Loan requirements] â”€â”€â”€â”€â”€â”€â”€â”€  0.76  â† Should be #2!\n",
    "```\n",
    "\n",
    "**After Reranking (Cross-Encoder):**\n",
    "```\n",
    "Retrieved 10, reranked, kept top-5:\n",
    "\n",
    "1. [Financing options] â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.94  âœ… NOW #1!\n",
    "2. [Loan requirements] â”€â”€â”€â”€â”€â”€â”€â”€  0.89  âœ… NOW #2!\n",
    "3. [Development Bank info] â”€â”€â”€â”€ 0.85\n",
    "4. [SMEDAN financing] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.82\n",
    "5. [BOI programs] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.78\n",
    "```\n",
    "\n",
    "**Result:** Much better document ranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_openai import setup_openai_api, create_embeddings, create_llm, load_msme_data, create_vectorstore, get_baseline_prompt, load_existing_vectorstore\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print('[OK] Imports done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = setup_openai_api()\n",
    "embeddings = create_embeddings(api_key)\n",
    "llm = create_llm(api_key)\n",
    "docs, metas, ids = load_msme_data('msme.csv')\n",
    "vectorstore = create_vectorstore(docs, metas, ids, embeddings, 'msme_t7', './chroma_db_t7')\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={'k': 10})  # Retrieve MORE for reranking\n",
    "print('[OK] Base retriever ready (k=10)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = setup_openai_api()\n",
    "embeddings = create_embeddings(api_key)\n",
    "llm = create_llm(api_key)\n",
    "vectorstore = load_existing_vectorstore(embeddings, 'msme_t7', './chroma_db_t7')\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={'k': 10})  # Retrieve MORE for reranking\n",
    "print('[OK] Base retriever ready (k=10)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Cross-Encoder Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Bi-Encoder vs Cross-Encoder: The Key Difference\n",
    "\n",
    "### Bi-Encoder (Used in Vector Search - Stage 1)\n",
    "```\n",
    "Query â†’ Embed â†’ Vector_Q  â”€â”\n",
    "                            â”œâ†’ cosine_similarity(Vector_Q, Vector_D)\n",
    "Doc â†’ Embed â†’ Vector_D    â”€â”˜\n",
    "```\n",
    "- **Process:** Query and document embedded **separately**\n",
    "- **Comparison:** Simple cosine similarity between vectors\n",
    "- **Speed:** FAST\n",
    "- **Accuracy:** Good (but misses nuanced relevance)\n",
    "\n",
    "### Cross-Encoder (Used in Reranking - Stage 2)\n",
    "```\n",
    "[Query + Doc] â†’ Feed TOGETHER into model â†’ Relevance Score\n",
    "```\n",
    "- **Process:** Query and document processed **together** as a pair\n",
    "- **Comparison:** Model sees both at once, captures interaction\n",
    "- **Speed:** SLOWER (must run model for each query-doc pair)\n",
    "- **Accuracy:** EXCELLENT (captures semantic relationships)\n",
    "\n",
    "**Why Cross-Encoder is More Accurate:**\n",
    "- Sees query and doc together (not in isolation)\n",
    "- Can capture word interactions and context\n",
    "- Optimized specifically for ranking tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using HuggingFace cross-encoder model:\n",
    "model = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "reranker = CrossEncoderReranker(model=model, top_n=5)  # Rerank and keep top 5\n",
    "print('[OK] Reranker created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Wrap with ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranking_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "print('[OK] Reranking retriever ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_baseline_prompt()\n",
    "\n",
    "reranking_rag_chain = (\n",
    "    {'context': reranking_retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print('[OK] Reranking RAG chain ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What are the challenges faced by MSMEs in Nigeria?'\n",
    "\n",
    "# Compare base retrieval order vs reranked order\n",
    "base_docs = base_retriever.invoke(question)\n",
    "reranked_docs = reranking_retriever.invoke(question)\n",
    "\n",
    "print(f'Base retrieval: {len(base_docs)} docs')\n",
    "print(f'After reranking: {len(reranked_docs)} docs')\n",
    "print(f'\\nTop doc after reranking:')\n",
    "print(reranked_docs[0].page_content[:300])\n",
    "\n",
    "answer = reranking_rag_chain.invoke(question)\n",
    "print(f'\\nAnswer:\\n{answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ Performance Trade-offs\n",
    "\n",
    "| Aspect | Vector Search Only | With Reranking |\n",
    "|--------|-------------------|----------------|\n",
    "| **Speed** | Very fast (~10ms) | Slower (~100-500ms) |\n",
    "| **Accuracy** | Good (70-80%) | Excellent (85-95%) |\n",
    "| **Cost** | Cheap | More expensive |\n",
    "| **Computation** | Pre-computed embeddings | Must score each pair |\n",
    "| **Use Case** | Speed-critical apps | Quality-critical tasks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use\n",
    "**Use when:**\n",
    "- Accuracy critical\n",
    "- Initial retrieval misses best docs\n",
    "- Can afford extra computation\n",
    "\n",
    "**Avoid when:**\n",
    "- Speed is priority\n",
    "- Initial retrieval already good\n",
    "- Extra cost unjustified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Compare answers with/without reranking\n",
    "2. Test different reranker models\n",
    "3. Measure quality improvement\n",
    "\n",
    "Time: 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next:** Technique 5 - HyDE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bao_env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
