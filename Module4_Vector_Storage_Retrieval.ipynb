{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Vector Storage & Retrieval\n",
    "\n",
    " **Level:** Intermediate  \n",
    "**Prerequisites:** Modules 1, 2, and 3 completed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "- Understand what vector databases are and why they're needed\n",
    "- Choose the right vector database for your use case\n",
    "- Implement vector storage using FAISS (local)\n",
    "- Implement vector storage using Chroma (embedded)\n",
    "- Perform efficient similarity search at scale\n",
    "- Add filtering and metadata to retrieval\n",
    "- Optimize retrieval performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Why Vector Databases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Problem with Simple Storage\n",
    "\n",
    "In Module 3, we stored embeddings in a Python list and did linear search:\n",
    "\n",
    "```python\n",
    "# Simple approach from Module 3\n",
    "embeddings = [emb1, emb2, emb3, ...]  # List of vectors\n",
    "\n",
    "# Search: compare query to EVERY vector\n",
    "for emb in embeddings:\n",
    "    similarity = cosine_similarity(query_emb, emb)\n",
    "```\n",
    "\n",
    "**This works for small datasets but fails at scale:**\n",
    "\n",
    "| Documents | Vectors | Search Time |\n",
    "|-----------|---------|-------------|\n",
    "| 100 | 100 | ~10ms |\n",
    "| 1,000 | 1,000 | ~100ms |\n",
    "| 10,000 | 10,000 | ~1 second |\n",
    "| 100,000 | 100,000 | ~10 seconds |\n",
    "| 1,000,000 | 1,000,000 | ~100 seconds |\n",
    "\n",
    "**Real-world RAG systems have millions of vectors. We need a better solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 What is a Vector Database?\n",
    "\n",
    "A vector database is a specialized database designed to:\n",
    "- Store high-dimensional vectors efficiently\n",
    "- Perform fast similarity search (even with millions of vectors)\n",
    "- Handle metadata alongside vectors\n",
    "- Support filtering and hybrid search\n",
    "\n",
    "**Key difference from regular databases:**\n",
    "- Regular DB: Exact match queries (`WHERE name = 'John'`)\n",
    "- Vector DB: Similarity queries (`Find vectors most similar to query vector`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Popular Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Comparison Table\n",
    "\n",
    "| Database | Type | Best For | Pros | Cons |\n",
    "|----------|------|----------|------|------|\n",
    "| **FAISS** | Library (local) | Prototyping, research | Fast, free, flexible | No server, no persistence by default |\n",
    "| **Chroma** | Embedded | Small to medium projects | Easy setup, built for LLMs | Limited scale |\n",
    "| **Pinecone** | Cloud (managed) | Production, scale | Fully managed, scalable | Costs money, cloud-only |\n",
    "| **Weaviate** | Self-hosted/Cloud | Production, flexibility | Feature-rich, open source | Complex setup |\n",
    "| **Qdrant** | Self-hosted/Cloud | Production, performance | Fast, great filtering | Requires setup |\n",
    "| **Milvus** | Self-hosted/Cloud | Large scale, enterprise | Very scalable, mature | Complex, resource-intensive |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decision Guide\n",
    "\n",
    "**Use FAISS when:**\n",
    "- Learning and prototyping\n",
    "- Running locally without server\n",
    "- Need maximum speed and flexibility\n",
    "- Don't need persistence (or can handle it yourself)\n",
    "\n",
    "**Use Chroma when:**\n",
    "- Building small to medium RAG apps\n",
    "- Want simplicity and easy setup\n",
    "- Need basic persistence and metadata\n",
    "- Working with LangChain or LlamaIndex\n",
    "\n",
    "**Use Pinecone when:**\n",
    "- Building production applications\n",
    "- Want fully managed service (no DevOps)\n",
    "- Need to scale to millions of vectors\n",
    "- Have budget for managed service\n",
    "\n",
    "**Use Weaviate/Qdrant when:**\n",
    "- Need production features but want to self-host\n",
    "- Want advanced filtering and hybrid search\n",
    "- Have DevOps resources\n",
    "\n",
    "**For this module, we'll focus on FAISS and Chroma (most common for learning).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Hands-On: FAISS Vector Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q faiss-cpu sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\",\n",
    "    \"Data visualization helps communicate insights from complex datasets.\",\n",
    "    \"Cloud computing provides on-demand access to computing resources.\",\n",
    "    \"Cybersecurity protects systems and networks from digital attacks.\",\n",
    "    \"Blockchain technology enables secure, decentralized transactions.\",\n",
    "    \"Quantum computing uses quantum mechanics to solve complex problems.\"\n",
    "]\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {embeddings.shape[1]} dimensions\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding dimension\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index (IndexFlatL2 = exact search with L2 distance)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"‚úÖ FAISS index created!\")\n",
    "print(f\"Total vectors in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Understanding FAISS Indexes\n",
    "\n",
    "**IndexFlatL2**: Exact search using L2 (Euclidean) distance. Best for small datasets or when you need perfect accuracy.\n",
    "\n",
    "**Other index types:**\n",
    "- `IndexFlatIP`: Exact search using inner product (similar to cosine similarity)\n",
    "- `IndexIVFFlat`: Approximate search using clustering (faster for large datasets)\n",
    "- `IndexHNSWFlat`: Graph-based approximate search (very fast)\n",
    "\n",
    "### What is the purpose of an index?\n",
    "üëâ 1. Store vectors efficiently\n",
    "\n",
    "The index is a special data structure that stores your embeddings (vector representations of text).\n",
    "\n",
    "üëâ 2. Allow fast similarity search\n",
    "\n",
    "Instead of scanning all vectors one by one (slow), the index uses algorithms to find the closest vectors very fast, even when you have millions.\n",
    "\n",
    "üëâ 3. Provide distance + nearest neighbors\n",
    "\n",
    "When you search with a query vector, the index returns:\n",
    "\n",
    "- I ‚Üí indices of the closest stored vectors\n",
    "- D ‚Üí distances showing how similar they are\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "I = [[5, 12, 3]]   # best matches\n",
    "D = [[0.12, 0.34, 0.89]]   # distances\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"What is artificial intelligence and machine learning?\"\n",
    "\n",
    "# Embed query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Search: find top 3 most similar vectors\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results:\\n\")\n",
    "\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0]), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   {documents[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Understanding Distances\n",
    "\n",
    "**L2 distance** (what IndexFlatL2 uses):\n",
    "- **Lower = More similar** (opposite of cosine similarity!)\n",
    "- 0 = Identical vectors\n",
    "- Higher values = More different\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Using Cosine Similarity with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create index with inner product (equivalent to cosine for normalized vectors)\n",
    "index_cosine = faiss.IndexFlatIP(dimension)\n",
    "index_cosine.add(embeddings_normalized)\n",
    "\n",
    "# Search with normalized query\n",
    "query_embedding_normalized = query_embedding / np.linalg.norm(query_embedding)\n",
    "scores, indices = index_cosine.search(query_embedding_normalized, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results with cosine similarity:\\n\")\n",
    "\n",
    "for i, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):\n",
    "    print(f\"{i}. (Similarity: {score:.4f})\")\n",
    "    print(f\"   {documents[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Saving and Loading FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index to disk\n",
    "faiss.write_index(index_cosine, \"my_faiss_index.bin\")\n",
    "print(\"‚úÖ Index saved to disk\")\n",
    "\n",
    "# Save documents separately (FAISS only stores vectors, not text)\n",
    "import pickle\n",
    "with open(\"documents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(documents, f)\n",
    "print(\"‚úÖ Documents saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index from disk\n",
    "loaded_index = faiss.read_index(\"my_faiss_index.bin\")\n",
    "print(f\"‚úÖ Index loaded: {loaded_index.ntotal} vectors\")\n",
    "\n",
    "# Load documents\n",
    "with open(\"documents.pkl\", \"rb\") as f:\n",
    "    loaded_documents = pickle.load(f)\n",
    "print(f\"‚úÖ Documents loaded: {len(loaded_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Hands-On: Chroma Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Why Chroma?\n",
    "\n",
    "**Chroma advantages over FAISS:**\n",
    "- Built specifically for LLM applications\n",
    "- Automatic persistence (saves to disk automatically)\n",
    "- Stores documents AND embeddings together\n",
    "- Rich metadata support and filtering\n",
    "\n",
    "**Trade-off:** Less flexible than FAISS, not as fast for very large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")\n",
    "print(\"‚úÖ ChromaDB imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Create Chroma Client and Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma client (persistent storage)\n",
    "# Note: ChromaDB 0.4.0+ uses PersistentClient instead of Client(Settings(...))\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create or get collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"my_documents\",\n",
    "    metadata={\"description\": \"Sample document collection\"}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Collection created: {collection.name}\")\n",
    "print(f\"Current count: {collection.count()} documents\")\n",
    "print(f\"üìÅ Data persisted to: ./chroma_db/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Add Documents to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents with metadata\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\"\n",
    "]\n",
    "\n",
    "# Metadata for each document\n",
    "metadatas = [\n",
    "    {\"category\": \"programming\", \"topic\": \"python\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"machine learning\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"neural networks\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"NLP\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"deep learning\"}\n",
    "]\n",
    "\n",
    "# IDs for each document\n",
    "ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Add to collection (Chroma handles embedding automatically!)\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Added {len(documents)} documents to collection\")\n",
    "print(f\"Total documents: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Chroma Magic\n",
    "\n",
    "Notice: We didn't manually generate embeddings! Chroma does it automatically using a default embedding model.\n",
    "\n",
    "**You can also specify your own embedding function (we'll see this later).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Query Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"Query: What is artificial intelligence?\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Filtering with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with metadata filter\n",
    "results = collection.query(\n",
    "    query_texts=[\"Tell me about AI\"],\n",
    "    n_results=3,\n",
    "    where={\"category\": \"AI\"}  # Only return AI documents\n",
    ")\n",
    "\n",
    "print(\"Query: Tell me about AI (filtered by category='AI')\\n\")\n",
    "print(\"Results:\\n\")\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "    print(f\"   Category: {metadata['category']}, Topic: {metadata['topic']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is how the filtering is being done in real life:\n",
    "\n",
    "```\n",
    "detected_category = classify_user_query(query)  # AI ‚Üí returns \"AI\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5,\n",
    "    where={\"category\": detected_category}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Using Custom Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Use sentence-transformers embedding function\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create new collection with custom embedding function\n",
    "collection_custom = client.get_or_create_collection(\n",
    "    name=\"custom_embeddings\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "collection_custom.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Collection with custom embeddings created\")\n",
    "print(f\"Documents: {collection_custom.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection_custom.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3,\n",
    "    include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection_custom.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3,\n",
    "    include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"Query: What is artificial intelligence?\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Update and Delete Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a document\n",
    "collection.update(\n",
    "    ids=[\"doc_0\"],\n",
    "    documents=[\"Python is an amazing programming language for AI and data science!\"],\n",
    "    metadatas=[{\"category\": \"programming\", \"topic\": \"python\", \"updated\": True}]\n",
    ")\n",
    "print(\"‚úÖ Document updated\")\n",
    "\n",
    "# Delete a document\n",
    "# collection.delete(ids=[\"doc_4\"])\n",
    "# print(\"‚úÖ Document deleted\")\n",
    "\n",
    "print(f\"\\nTotal documents after update: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Building a Complete RAG Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 RAG Retriever with Chroma\n",
    "\n",
    "Let's combine everything: chunking (Module 2), embeddings (Module 3), and vector storage (Module 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class RAGRetriever:\n",
    "    def __init__(self, collection_name=\"rag_collection\", persist_dir=\"./rag_db\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG retriever with Chroma.\n",
    "        \"\"\"\n",
    "        # Create Chroma client (using PersistentClient for ChromaDB 0.4.0+)\n",
    "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
    "        \n",
    "        # Create collection with sentence-transformers\n",
    "        embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=embedding_fn\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ RAG Retriever initialized\")\n",
    "        print(f\"Collection: {collection_name}\")\n",
    "        print(f\"Current documents: {self.collection.count()}\")\n",
    "        print(f\"üìÅ Data persisted to: {persist_dir}/\")\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=500):\n",
    "        \"\"\"\n",
    "        Simple sentence-based chunking from Module 2.\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def add_document(self, text, metadata=None, source_name=\"unknown\"):\n",
    "        \"\"\"\n",
    "        Add a document (chunks it automatically).\n",
    "        \"\"\"\n",
    "        # Chunk the document\n",
    "        chunks = self.chunk_text(text)\n",
    "        \n",
    "        # Prepare data for Chroma\n",
    "        ids = [f\"{source_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source\": source_name,\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                **(metadata or {})\n",
    "            }\n",
    "            for i in range(len(chunks))\n",
    "        ]\n",
    "        \n",
    "        # Add to collection\n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Added document '{source_name}': {len(chunks)} chunks\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def retrieve(self, query, top_k=3, filter_metadata=None):\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for a query.\n",
    "        \"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k,\n",
    "            where=filter_metadata\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'documents': results['documents'][0],\n",
    "            'metadatas': results['metadatas'][0],\n",
    "            'distances': results['distances'][0]\n",
    "        }\n",
    "    \n",
    "    def format_context(self, retrieved_results):\n",
    "        \"\"\"\n",
    "        Format retrieved chunks for LLM prompt.\n",
    "        \"\"\"\n",
    "        context = \"Context from retrieved documents:\\n\\n\"\n",
    "        \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            retrieved_results['documents'],\n",
    "            retrieved_results['metadatas'],\n",
    "            retrieved_results['distances']\n",
    "        ), 1):\n",
    "            source = metadata.get('source', 'unknown')\n",
    "            context += f\"[{i}] From {source} (Relevance: {1/(1+distance):.3f}):\\n\"\n",
    "            context += f\"{doc}\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "print(\"‚úÖ RAGRetriever class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Test the RAG Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = RAGRetriever(collection_name=\"test_rag\")\n",
    "\n",
    "# Add sample documents\n",
    "doc1 = \"\"\"\n",
    "Machine learning is a branch of artificial intelligence that focuses on building systems \n",
    "that can learn from data. These systems improve their performance over time without being \n",
    "explicitly programmed. Common applications include image recognition, natural language \n",
    "processing, and recommendation systems.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "Python is a high-level programming language known for its simplicity and readability. \n",
    "It's widely used in web development, data science, automation, and artificial intelligence. \n",
    "Python's extensive library ecosystem makes it ideal for rapid development.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Vector databases are specialized databases designed to store and query high-dimensional \n",
    "vectors efficiently. They're essential for modern AI applications like semantic search, \n",
    "recommendation systems, and retrieval-augmented generation (RAG). Popular examples include \n",
    "FAISS, Pinecone, and Chroma.\n",
    "\"\"\"\n",
    "\n",
    "# Add documents\n",
    "retriever.add_document(doc1, metadata={\"category\": \"AI\"}, source_name=\"ml_intro.txt\")\n",
    "retriever.add_document(doc2, metadata={\"category\": \"programming\"}, source_name=\"python_guide.txt\")\n",
    "retriever.add_document(doc3, metadata={\"category\": \"databases\"}, source_name=\"vector_db_overview.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"What are vector databases used for?\"\n",
    "\n",
    "results = retriever.retrieve(query, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(retriever.format_context(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Advanced Retrieval Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Reranking\n",
    "\n",
    "Retrieve more candidates (e.g., top 20), then rerank them using a more sophisticated model to get the final top k.\n",
    "\n",
    "**Benefits:**\n",
    "- Better quality than single-stage retrieval\n",
    "- Can use cross-encoder models (more accurate but slower)\n",
    "\n",
    "**How it works:**\n",
    "1. Retrieval: Get top 20 candidates (fast, approximate)\n",
    "2. Reranking: Score all 20 with better model\n",
    "3. Return: Top 3 after reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Hybrid Search\n",
    "\n",
    "Combine semantic search (embeddings) with keyword search (BM25, TF-IDF).\n",
    "\n",
    "**Why hybrid?**\n",
    "- Semantic search: Good for concepts and meaning\n",
    "- Keyword search: Good for exact terms and names\n",
    "- Together: Best of both worlds\n",
    "\n",
    "**Implementation:**\n",
    "1. Get results from semantic search\n",
    "2. Get results from keyword search\n",
    "3. Combine and rerank (e.g., weighted average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 MMR (Maximal Marginal Relevance)\n",
    "\n",
    "Retrieve diverse results instead of very similar ones.\n",
    "\n",
    "**Problem:** Top 3 results might be too similar (redundant)\n",
    "\n",
    "**Solution:** MMR balances relevance and diversity\n",
    "- Pick most relevant document first\n",
    "- For next picks, balance relevance to query vs. difference from already selected docs\n",
    "\n",
    "**Use when:** You want variety in retrieved context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Index Selection\n",
    "\n",
    "Choose the right FAISS index based on your needs:\n",
    "\n",
    "**For accuracy (small datasets < 10k vectors):**\n",
    "- `IndexFlatL2` or `IndexFlatIP`: Exact search, no approximation\n",
    "\n",
    "**For speed (medium datasets 10k-1M vectors):**\n",
    "- `IndexIVFFlat`: Clusters data, searches subset\n",
    "- `IndexHNSWFlat`: Graph-based, very fast\n",
    "\n",
    "**For memory (large datasets > 1M vectors):**\n",
    "- `IndexIVFPQ`: Compressed vectors, saves memory\n",
    "- Trade-off: Faster and smaller, but less accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Batch Processing\n",
    "\n",
    "Add documents in batches instead of one by one for better performance.\n",
    "\n",
    "```python\n",
    "# Slow: Adding one at a time\n",
    "for doc in documents:\n",
    "    collection.add(documents=[doc], ids=[...])\n",
    "\n",
    "# Fast: Adding in batch\n",
    "collection.add(documents=documents, ids=[...])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Dimension Reduction\n",
    "\n",
    "Reduce embedding dimensions to save memory and improve speed.\n",
    "\n",
    "**Example:** 768 dims ‚Üí 384 dims or 256 dims\n",
    "\n",
    "**Trade-off:** Faster and smaller, slightly lower quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Practice Exercises\n",
    "\n",
    "## Exercise 1: Chroma with Advanced Filtering\n",
    "\n",
    "### Task\n",
    "Build a document management system using Chroma with rich metadata and filtering.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Create a collection of at least 30 documents with metadata:\n",
    "   ```python\n",
    "   metadata = {\n",
    "       \"category\": \"...\",  # e.g., \"tech\", \"business\", \"science\"\n",
    "       \"date\": \"...\",      # e.g., \"2024-01-15\"\n",
    "       \"author\": \"...\",    # e.g., \"John Doe\"\n",
    "       \"priority\": ...    # e.g., 1, 2, 3\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. Implement queries with different filters:\n",
    "   - By category\n",
    "   - By date range\n",
    "   - By author\n",
    "   - Combined filters (e.g., category AND date)\n",
    "\n",
    "3. Test MMR (Maximal Marginal Relevance) if Chroma supports it\n",
    "\n",
    "4. Compare results with and without filters\n",
    "\n",
    "### Sample Data\n",
    "\n",
    "```python\n",
    "documents = [\n",
    "    {\n",
    "        \"text\": \"Python 3.12 introduces new performance improvements...\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"tech\",\n",
    "            \"date\": \"2024-01-15\",\n",
    "            \"author\": \"Tech Team\",\n",
    "            \"priority\": 1\n",
    "        }\n",
    "    },\n",
    "    # Add 29 more...\n",
    "]\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "```\n",
    "Query: \"latest technology updates\"\n",
    "\n",
    "Without filters:\n",
    "1. [Result from any category]\n",
    "2. [Result from any category]\n",
    "3. [Result from any category]\n",
    "\n",
    "With filter (category=\"tech\"):\n",
    "1. [Tech result]\n",
    "2. [Tech result]\n",
    "3. [Tech result]\n",
    "\n",
    "With filter (category=\"tech\" AND date>=\"2024-01-01\"):\n",
    "1. [Recent tech result]\n",
    "2. [Recent tech result]\n",
    "3. [Recent tech result]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Vector databases enable fast similarity search** at scale using approximate nearest neighbor algorithms.\n",
    "\n",
    "2. **FAISS** is great for learning, prototyping, and maximum flexibility. Requires manual persistence.\n",
    "\n",
    "3. **Chroma** is perfect for RAG applications with automatic persistence, metadata support, and simple API.\n",
    "\n",
    "4. **Production systems** typically use managed services like Pinecone or self-hosted solutions like Weaviate/Qdrant.\n",
    "\n",
    "5. **Metadata filtering** allows you to narrow search to specific document types or categories.\n",
    "\n",
    "6. **Advanced techniques** like reranking, hybrid search, and MMR improve retrieval quality.\n",
    "\n",
    "7. **Choose the right index** based on your dataset size and accuracy requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "In **Module 5: Building Your First Complete RAG System**, you'll:\n",
    "- Combine all modules into a working RAG application\n",
    "- Add an LLM for generation\n",
    "- Handle document uploads\n",
    "- Create a simple interface\n",
    "- Test with real queries\n",
    "\n",
    "Get ready to build a complete system! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "publica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
