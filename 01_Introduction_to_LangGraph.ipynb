{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1: Introduction to LangGraph & State Management\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what LangGraph is and why we need it\n",
    "- Learn core concepts: nodes, edges, state, checkpointers\n",
    "- Build a simple stateful chatbot with conversation memory\n",
    "- Compare LangGraph with LangChain chains\n",
    "\n",
    "**Prerequisites:** RAG with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: What is LangGraph?\n",
    "\n",
    "### The Problem with Chains\n",
    "\n",
    "You have learned about **LangChain chains** - they're great for fixed pipelines:\n",
    "\n",
    "```python\n",
    "# Always follows this path:\n",
    "chain = retriever | prompt | llm | parser\n",
    "```\n",
    "\n",
    "But what if you need:\n",
    "- **Decisions during execution?** (\"Should I retrieve or not?\")\n",
    "- **Loops and cycles?** (\"Try again if answer is poor\")\n",
    "- **Multiple tools?** (\"Use search OR calculator OR retrieval\")\n",
    "- **Complex control flow?** (\"If X then Y, else Z\")\n",
    "\n",
    "**That's where LangGraph comes in!**\n",
    "\n",
    "### LangGraph = State Machines for Agents\n",
    "\n",
    "LangGraph lets you build **graphs** where:\n",
    "- **Nodes** are functions that process state\n",
    "- **Edges** connect nodes (fixed or conditional)\n",
    "- **State** flows through the graph\n",
    "- **Agents make decisions** about which path to take\n",
    "\n",
    "```\n",
    "           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  START ‚îÄ‚îÄ‚ñ∂‚îÇ  Node 1  ‚îÇ\n",
    "           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ\n",
    "                ‚ñº\n",
    "           Decision?\n",
    "           ‚îú‚îÄ YES ‚îÄ‚ñ∂ Node 2 ‚îÄ‚îÄ‚îê\n",
    "           ‚îî‚îÄ NO  ‚îÄ‚ñ∂ Node 3 ‚îÄ‚îÄ‚î§\n",
    "                               ‚ñº\n",
    "                              END\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangGraph Application Architecture](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/application.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q langgraph langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up OpenAI API Key\n",
    "\n",
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM\n",
    "\n",
    "We'll use **GPT-4o-mini** - it's fast and cost-effective for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Core Concept #1 - State\n",
    "\n",
    "**State** ‚ÄúThe single source of truth for the whole agent execution.‚Äù  \n",
    "**State** is the data that flows through your graph.\n",
    "\n",
    "### MessagesState\n",
    "\n",
    "For chatbots, LangGraph provides `MessagesState` - it stores conversation history:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hello\"),\n",
    "        AIMessage(content=\"Hi! How can I help?\"),\n",
    "        HumanMessage(content=\"What's Python?\"),\n",
    "        # ... more messages\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is similar to `ConversationBufferMemory`, but more flexible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Reflection Question:** \n",
    "How is this different from ConversationBufferMemory? In chains, memory was managed separately. In LangGraph, it's part of the state that flows through nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Concept #2 - Nodes\n",
    "\n",
    "**Nodes** are functions that process state and return updates.\n",
    "\n",
    "### Node Function Signature\n",
    "\n",
    "```python\n",
    "def my_node(state: MessagesState) -> dict:\n",
    "    # Process state\n",
    "    # Return updates to state\n",
    "    return {\"messages\": [new_message]}\n",
    "```\n",
    "\n",
    "### The Assistant Node\n",
    "\n",
    "Let's create our first node - it sends messages to the LLM and gets a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt that defines assistant behavior\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a friendly assistant that answers user questions. Be helpful and concise.\"\n",
    ")\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    The assistant node - processes messages and generates response.\n",
    "    \"\"\"\n",
    "    # Combine system prompt with conversation history\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return as state update\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "print(\"‚úÖ Assistant node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Point:** The node doesn't modify state directly - it returns updates that LangGraph applies automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Retriever Node\n",
    "You don't need to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_docs(state: MessagesState):\n",
    "#     query = state[\"messages\"][-1].content  # latest HumanMessage\n",
    "#     docs = retriever.invoke(query)\n",
    "\n",
    "#     return {\n",
    "#         \"messages\": [\n",
    "#             ToolMessage(\n",
    "#                 content=\"\\n\".join(d.page_content for d in docs),\n",
    "#                 name=\"retriever\"\n",
    "#             )\n",
    "#         ]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Concept 2B - Edges\n",
    "\n",
    "**Edges** are the connections between nodes that control the flow of your agent.\n",
    "\n",
    "Think of edges as **roads** between cities (nodes). They determine which node to visit next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Edges\n",
    "\n",
    "LangGraph has two types of edges:\n",
    "\n",
    "1. **Fixed/Static Edges** (Normal Edges)\n",
    "   - Always go from Node A to Node B\n",
    "   - No decision-making\n",
    "   - Used for predictable flows\n",
    "\n",
    "2. **Conditional Edges**\n",
    "   - Decide which node to visit next based on state\n",
    "   - Enable agent decision-making\n",
    "   - Used for dynamic, intelligent behavior\n",
    "\n",
    "```\n",
    "Fixed/Static Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Node B  (always goes to B)\n",
    "\n",
    "Conditional Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Decision?\n",
    "                    ‚îú‚îÄ Condition 1 ‚îÄ‚îÄ‚ñ∂ Node B\n",
    "                    ‚îú‚îÄ Condition 2 ‚îÄ‚îÄ‚ñ∂ Node C\n",
    "                    ‚îî‚îÄ Condition 3 ‚îÄ‚îÄ‚ñ∂ Node D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Core Concept #3 - Building the Graph\n",
    "\n",
    "Now let's connect everything into a **StateGraph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StateGraph with MessagesState\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add the assistant node\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "# Define the flow:\n",
    "# START ‚Üí assistant ‚Üí END\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_edge(\"assistant\", END)\n",
    "\n",
    "print(\"‚úÖ Graph structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Flow\n",
    "\n",
    "```\n",
    "START ‚Üí [assistant node] ‚Üí END\n",
    "```\n",
    "\n",
    "- **START:** Entry point (receives user message)\n",
    "- **assistant:** Processes message and generates response\n",
    "- **END:** Exit point (returns final state)\n",
    "\n",
    "This is simple now, but later we'll add conditional edges for agentic behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Core Concept #4 - Checkpointers (Memory)\n",
    "\n",
    "**Checkpointers** save state between interactions - this gives our agent memory!\n",
    "\n",
    "Without checkpointer:\n",
    "- Each call starts fresh\n",
    "- No conversation history\n",
    "- Agent forgets everything\n",
    "\n",
    "With checkpointer:\n",
    "- State persists between calls\n",
    "- Agent remembers conversation\n",
    "- Multi-turn conversations work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory checkpointer (stores in memory)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph WITH memory\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Agent compiled with memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîß Production Note:** `MemorySaver` stores in RAM (lost on restart). For production, use:\n",
    "- `SqliteSaver` - persists to SQLite database\n",
    "- `MongoDBSaver` - persists to MongoDB\n",
    "\n",
    "We'll use MemorySaver for learning since it's simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Visualizing the Graph\n",
    "\n",
    "One of LangGraph's best features - **visual representation** of your agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph structure: START ‚Üí assistant ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Pro Tip:** Always visualize your graph! It helps you understand and debug agent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Running the Agent\n",
    "\n",
    "Now let's actually use our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session IDs (Thread IDs)\n",
    "\n",
    "Each conversation has a unique **thread_id**. Messages with the same thread_id share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a session ID for this conversation\n",
    "session_id = \"chat-session-0012\"\n",
    "\n",
    "print(f\"Starting conversation with session ID: {session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(user_input: str, thread_id: str = session_id):\n",
    "    \"\"\"\n",
    "    Send a message to the agent and get response.\n",
    "    ‚ö†Ô∏è WARNING: Using default thread_id shares conversation acrosss all calls!\n",
    "    In production, ALWAYS provide unique thread_id per user.\n",
    "    \"\"\"\n",
    "    # Invoke the agent\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    # Print the conversation\n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nüë§ User: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"ü§ñ Agent: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Conversation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Single Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_conversation(\"Hello! What's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Multi-Turn Conversation (Memory Test!)\n",
    "\n",
    "Now let's test if the agent remembers context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First message\n",
    "run_conversation(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question - does it remember?\n",
    "run_conversation(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_conversation(\"What's my favorite color?\", thread_id=\"111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Success!** The agent remembered your favorite color because:\n",
    "1. The checkpointer saved the state after the first message\n",
    "2. The same thread_id retrieved that saved state\n",
    "3. The conversation history was passed to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Context-Dependent Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new topic\n",
    "run_conversation(\"I'm learning about RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference it\n",
    "run_conversation(\"Can you explain the main components?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question\n",
    "run_conversation(\"Which component is most important?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Notice:** The agent maintains context across multiple turns - just like a real conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Multiple Conversations (Different Thread IDs)\n",
    "\n",
    "Let's test that different thread IDs have separate memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation 1\n",
    "print(\"\\nüîµ CONVERSATION 1\")\n",
    "run_conversation(\"My name is Alice\", thread_id=\"user_alicee\")\n",
    "\n",
    "# Conversation 2 (different user)\n",
    "print(\"\\nüü¢ CONVERSATION 2\")\n",
    "run_conversation(\"My name is Bob\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to Alice - does it remember her name?\n",
    "print(\"\\nüîµ BACK TO CONVERSATION 1\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_alicee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to Bob\n",
    "print(\"\\nüü¢ BACK TO CONVERSATION 2\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Key Insight:** Each thread_id maintains its own conversation history. This is how you'd handle multiple users in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Interactive Chat Loop\n",
    "\n",
    "Let's create an interactive chat session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session.\n",
    "    Type 'exit' or 'quit' to stop.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ü§ñ Interactive Chat Started\")\n",
    "    print(\"Type your message and press Enter. Type 'exit' to quit.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    thread_id = \"interactive_session2\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nüëã Goodbye!\\n\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get response\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=user_input)]},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "        )\n",
    "        \n",
    "        # Print agent's response\n",
    "        agent_message = result[\"messages\"][-1]\n",
    "        print(f\"\\nü§ñ Agent: {agent_message.content}\")\n",
    "\n",
    "# Uncomment to run interactive chat:\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: ConversationalRetrievalChain\n",
    "\n",
    "Let's compare LangGraph with the memory you learned in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain\n",
    "\n",
    "```python\n",
    "# Langchain approach\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is Python?\"})\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Simple to use\n",
    "- ‚úÖ Built-in memory\n",
    "- ‚ùå Fixed pipeline (always retrieves)\n",
    "- ‚ùå No conditional logic\n",
    "- ‚ùå Can't add complex decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 12: LangGraph Agent\n",
    "\n",
    "```python\n",
    "# LangGraph approach\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"assistant\", assistant_node)\n",
    "memory = MemorySaver()\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is Python?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"user_123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Flexible - add any nodes/edges\n",
    "- ‚úÖ Conditional logic (coming in Topic 2)\n",
    "- ‚úÖ Agents can make decisions\n",
    "- ‚úÖ Supports cycles and loops\n",
    "- ‚ùå More complex to set up\n",
    "- ‚ùå Requires understanding graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What?\n",
    "\n",
    "| Use Case | LangChains | LangGraph |\n",
    "|----------|-------------------|------------------------|\n",
    "| Simple chatbot | ‚úÖ Perfect | ‚ö†Ô∏è Overkill |\n",
    "| Fixed RAG pipeline | ‚úÖ Great | ‚ö†Ô∏è Unnecessary |\n",
    "| Agent with tools | ‚ùå Limited | ‚úÖ Ideal |\n",
    "| Conditional retrieval | ‚ùå Can't do | ‚úÖ Perfect |\n",
    "| Multi-agent systems | ‚ùå Not possible | ‚úÖ Built for it |\n",
    "\n",
    "**Rule of thumb:** If you need decision-making during execution, use LangGraph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **LangGraph Basics**\n",
    "   - LangGraph enables agentic behavior through graphs\n",
    "   - Better than chains when you need decisions during execution\n",
    "\n",
    "2. **Core Concepts**\n",
    "   - **State:** Data flowing through the graph (MessagesState for chat)\n",
    "   - **Nodes:** Functions that process and update state\n",
    "   - **Edges:** Connections between nodes (fixed or conditional)\n",
    "   - **Checkpointers:** Persist state for memory across sessions\n",
    "\n",
    "3. **Practical Skills**\n",
    "   - Built a stateful chatbot\n",
    "   - Used thread_id for multi-user conversations\n",
    "   - Visualized graph structure\n",
    "   - Compared with Module 9 chains\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Topic 2: Tool Integration**\n",
    "- Add tools for agents (search, calculator, retrieval)\n",
    "- Conditional edges (agent decides which tool to use)\n",
    "- This is where LangGraph really shines!\n",
    "\n",
    "**Topic 3: Agentic RAG**\n",
    "- Agent that decides when to retrieve\n",
    "- Combines everything from Topics 1-2\n",
    "- The core concept of this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "## Exercise 1: Build Your First Stateful Agent\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "**Estimated Time:** 30-45 minutes\n",
    "\n",
    "### Task\n",
    "Build a simple customer support chatbot that remembers conversation context.\n",
    "\n",
    "### Requirements\n",
    "1. Create a StateGraph with MessagesState\n",
    "2. Add a system prompt that makes the agent act as a helpful customer support rep\n",
    "3. Use MemorySaver checkpointer for memory\n",
    "4. Test with a multi-turn conversation where context matters\n",
    "\n",
    "### Example Conversation\n",
    "```\n",
    "User: \"I bought a laptop last week\"\n",
    "Agent: \"I'd be happy to help with your laptop! What seems to be the issue?\"\n",
    "User: \"It won't turn on\"\n",
    "Agent: \"I understand your laptop won't turn on. Have you tried...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "1. **How does LangGraph's state management differ from ConversationalRetrievalChain memory?**\n",
    "   \n",
    "2. **Why do we need thread_id for conversations?**\n",
    "   \n",
    "3. **What happens if you don't configure a checkpointer?**\n",
    "   \n",
    "4. **When would you choose chains over LangGraph?**\n",
    "   \n",
    "5. **How would you debug an agent that's not behaving correctly?**\n",
    "\n",
    "Write your answers below or discuss with your study group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**üéâ Topic 1 Complete!** \n",
    "\n",
    "You now understand LangGraph fundamentals. Next up: **Tool Integration** - where agents become truly powerful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
