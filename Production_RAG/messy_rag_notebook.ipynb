{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your First Complete RAG System with LangChain\n",
    "\n",
    "Welcome! In this notebook, we'll build a **complete Retrieval-Augmented Generation (RAG)** system using **LangChain** and **OpenAI**.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG combines information retrieval with language models to answer questions based on your own documents.\n",
    "\n",
    "## The Complete RAG Pipeline:\n",
    "\n",
    "1. **Load** documents \n",
    "2. **Chunk** them into smaller pieces\n",
    "3. **Embed** chunks into vectors\n",
    "4. **Store** in vector database\n",
    "5. **Retrieve** relevant chunks based on query\n",
    "6. **Generate** answers using an LLM with retrieved context\n",
    "\n",
    "Let's build it step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-openai chromadb sentence-transformers tiktoken openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up OpenAI API Key\n",
    "\n",
    "You'll need an OpenAI API key. Get one at: https://platform.openai.com/api-keys\n",
    "\n",
    "**Create a `.env` file** in this directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your-key-here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file\n",
    "# Create a .env file in this directory with: OPENAI_API_KEY=your-key-here\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"✅ API key loaded!\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: OPENAI_API_KEY not found in .env file\")\n",
    "    print(\"Create a .env file with: OPENAI_API_KEY=your-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import LangChain Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3 documents\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    'documents/',\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"✅ Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 10 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"✅ Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Embeddings\n",
    "\n",
    "We'll use OpenAI's embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"What is RAG?\")\n",
    "print(f\"✅ Embedding model loaded!\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "✅ Vector store created with 10 chunks!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vector store...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(f\"✅ Vector store created with {len(chunks)} chunks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7b: OR Load Existing Vector Store (Skip Step 4-7 if using this)\n\n**If you've already created the vector store**, you can load it directly instead of recreating it:\n\nThis is useful when you:\n- Want to re-run the notebook without rebuilding the index\n- Already have the embeddings saved\n- Just want to query the existing data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load existing vector store (uncomment to use)\n# vectorstore = Chroma(\n#     persist_directory=\"./chroma_db\",\n#     embedding_function=embeddings\n# )\n# print(f\"✅ Loaded existing vector store!\")\n\n# To use this:\n# 1. Comment out Steps 4-7 above (or just don't run them)\n# 2. Uncomment the code above\n# 3. Run this cell instead",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7c: Smart Approach - Load if Exists, Create if Not\n\n**Best practice**: Check if the vector store already exists:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Smart approach - load if exists, create if not (uncomment to use)\n# import os\n# \n# if os.path.exists(\"./chroma_db\"):\n#     print(\"Loading existing vector store...\")\n#     vectorstore = Chroma(\n#         persist_directory=\"./chroma_db\",\n#         embedding_function=embeddings\n#     )\n#     print(\"✅ Loaded existing vector store!\")\n# else:\n#     print(\"Creating new vector store...\")\n#     vectorstore = Chroma.from_documents(\n#         documents=chunks,\n#         embedding=embeddings,\n#         persist_directory=\"./chroma_db\"\n#     )\n#     print(f\"✅ Vector store created with {len(chunks)} chunks!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Set Up OpenAI LLM\n",
    "\n",
    "We'll use GPT-3.5-turbo for fast, cost-effective answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,  # 0 = more focused, 1 = more creative\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "print(\"✅ OpenAI LLM initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Retrieval Only\n",
    "\n",
    "Before we do full generation, let's see what documents we retrieve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is machine learning?\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "1. There are three main types of machine learning:\n",
      "\n",
      "1. Supervised Learning: The algorithm learns from labeled training data. Common applications include classification and regression tasks. Examples incl...\n",
      "\n",
      "2. Machine Learning Fundamentals\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It focuses on developing algorithm...\n",
      "\n",
      "3. 3. Reinforcement Learning: The algorithm learns through trial and error by receiving rewards or penalties. This is commonly used in robotics, game playing, and autonomous vehicles.\n",
      "\n",
      "Key concepts in ma...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is machine learning?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. There are three main types of machine learning:\n",
      "\n",
      "1. Supervised Learning: The algorithm learns from labeled training data. Common applications include classification and regression tasks. Examples include email spam detection and house price prediction.\n",
      "\n",
      "2. Unsupervised Learning: The algorithm finds patterns in unlabeled data. Clustering and dimensionality reduction are common techniques. Applications include customer segmentation and anomaly detection.\n",
      "====================\n",
      "2. Machine Learning Fundamentals\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It focuses on developing algorithms that can identify patterns and make decisions based on input data.\n",
      "\n",
      "There are three main types of machine learning:\n",
      "====================\n",
      "3. 3. Reinforcement Learning: The algorithm learns through trial and error by receiving rewards or penalties. This is commonly used in robotics, game playing, and autonomous vehicles.\n",
      "\n",
      "Key concepts in machine learning include training data, test data, features, labels, and model evaluation metrics such as accuracy, precision, and recall.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create the RAG Prompt Template\n",
    "\n",
    "This prompt tells the LLM how to use the retrieved context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prompt template created!\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"You are a helpful assistant answering questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question based on the context above. If you cannot answer based on the context, say so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(\"✅ Prompt template created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Build the RAG Chain\n",
    "\n",
    "This combines retrieval + generation into one pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG chain built!\n"
     ]
    }
   ],
   "source": [
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG chain built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test the Complete RAG System!\n",
    "\n",
    "Now let's ask questions and get AI-generated answers based on our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is machine learning?\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "============================================================\n",
      "ANSWER:\n",
      "============================================================\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It focuses on developing algorithms that can identify patterns and make decisions based on input data.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"What is machine learning?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Generating answer...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ANSWER:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(answer)\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Try More Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How do REST APIs work?\n",
      "\n",
      "A: REST APIs work by using standard HTTP methods (GET, POST, PUT, DELETE) to interact with resources on a server. Clients make requests to specific endpoints, providing headers, body data (for POST/PUT requests), and parameters as needed. The server processes these requests and responds with the appropriate HTTP status code to indicate the outcome of the operation. Data exchange typically occurs in JSON format, allowing for easy communication between different software applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "question2 = \"How do REST APIs work?\"\n",
    "answer2 = rag_chain.invoke(question2)\n",
    "\n",
    "print(f\"Q: {question2}\\n\")\n",
    "print(f\"A: {answer2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is quantum computing?\n",
      "\n",
      "A: Quantum computing is not directly related to the context provided on machine learning fundamentals.\n",
      "\n",
      "============================================================\n",
      "Expected: The AI should say something like:\n",
      "'I cannot answer this question based on the provided context.'\n",
      "or\n",
      "'The context does not contain information about quantum computing.'\n"
     ]
    }
   ],
   "source": [
    "# Question 4 - Test with something NOT in documents\n",
    "question4 = \"What is quantum computing?\"\n",
    "answer4 = rag_chain.invoke(question4)\n",
    "\n",
    "print(f\"Q: {question4}\\n\")\n",
    "print(f\"A: {answer4}\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Expected: The AI should say something like:\")\n",
    "print(\"'I cannot answer this question based on the provided context.'\")\n",
    "print(\"or\")\n",
    "print(\"'The context does not contain information about quantum computing.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Understanding the Complete System\n",
    "\n",
    "Congratulations! You've built a **complete RAG system** that:\n",
    "\n",
    "✅ Loads and chunks documents  \n",
    "✅ Generates embeddings  \n",
    "✅ Stores in vector database  \n",
    "✅ Retrieves relevant context  \n",
    "✅ **Generates AI answers using GPT-3.5**  \n",
    "\n",
    "## Why This Is Powerful:\n",
    "\n",
    "- LLM answers based on YOUR data\n",
    "- Works with private/domain-specific documents\n",
    "- No need to fine-tune the LLM\n",
    "- Combines semantic search + generation\n",
    "- Uses powerful GPT models\n",
    "\n",
    "## But Still Has Limitations:\n",
    "\n",
    "- ❌ Everything hardcoded in notebook\n",
    "- ❌ Can't reuse as a module\n",
    "- ❌ No API for applications to use\n",
    "- ❌ Not production-ready\n",
    "\n",
    "**Next step:** Refactor this into clean Python modules and create an API!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bao_env (Python 3.10)",
   "language": "python",
   "name": "bao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}