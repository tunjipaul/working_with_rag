{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Document Processing & Chunking\n",
    "  \n",
    "**Level:** Beginner to Intermediate  \n",
    "**Prerequisites:** Module 1 completed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "- Explain why chunking is necessary in RAG systems\n",
    "- Implement basic chunking strategies from scratch\n",
    "- Choose appropriate chunk sizes for different use cases\n",
    "- Understand the trade-offs between different chunking approaches\n",
    "- Preserve and use document metadata effectively\n",
    "- Handle different document formats (text, PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Why Chunking Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Problem with Whole Documents\n",
    "\n",
    "Imagine you have a 50-page legal contract and someone asks: *\"What is the termination notice period?\"*\n",
    "\n",
    "**Problems with using the whole document:**\n",
    "\n",
    "1. **Too long for context windows**\n",
    "   - Most LLMs have limited context (4K-128K tokens)\n",
    "   - A 50-page document might be 20,000+ tokens\n",
    "   - Can't fit multiple documents in one prompt\n",
    "\n",
    "2. **Poor retrieval precision**\n",
    "   - Embedding a whole document loses specificity\n",
    "   - Can't pinpoint exact relevant section\n",
    "   - Answer might be buried in irrelevant content\n",
    "\n",
    "3. **Inefficient and costly**\n",
    "   - Processing entire documents is slow\n",
    "   - Expensive in terms of tokens/API costs\n",
    "   - Wastes context window space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 How Chunking Solves This\n",
    "\n",
    "**Chunking = Breaking documents into smaller, meaningful pieces**\n",
    "\n",
    "**Benefits:**\n",
    "- Each chunk fits in context window\n",
    "- More precise retrieval (find exact relevant section)\n",
    "- Better embeddings (more specific semantic meaning)\n",
    "- Faster and cheaper processing\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "50-page contract\n",
    "      â†“\n",
    "Split into 100 chunks (500 words each)\n",
    "      â†“\n",
    "Embed each chunk separately\n",
    "      â†“\n",
    "Retrieve only the 3 most relevant chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Understanding Chunk Size\n",
    "\n",
    "The most important decision in chunking: **How big should chunks be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Chunk Size Trade-off\n",
    "\n",
    "### Small Chunks (100-300 tokens)\n",
    "\n",
    "**Pros:**\n",
    "- Very precise retrieval\n",
    "- Specific answers to specific questions\n",
    "- Less noise in retrieved context\n",
    "\n",
    "**Cons:**\n",
    "- May lose important surrounding context\n",
    "- Might split related information\n",
    "- Need to retrieve more chunks to get full picture\n",
    "\n",
    "**Best for:** FAQ systems, specific fact lookup\n",
    "\n",
    "---\n",
    "\n",
    "### Medium Chunks (300-600 tokens)\n",
    "\n",
    "**Pros:**\n",
    "- Good balance of precision and context\n",
    "- Usually keeps related information together\n",
    "- Most versatile option\n",
    "\n",
    "**Cons:**\n",
    "- Middle ground = not optimized for any specific case\n",
    "\n",
    "**Best for:** Most general RAG use cases (recommended starting point)\n",
    "\n",
    "---\n",
    "\n",
    "### Large Chunks (600-1000+ tokens)\n",
    "\n",
    "**Pros:**\n",
    "- Preserves more context\n",
    "- Better for complex, interconnected information\n",
    "- Fewer chunks to manage\n",
    "\n",
    "**Cons:**\n",
    "- Less precise retrieval\n",
    "- More irrelevant information included\n",
    "- May exceed context limits with multiple chunks\n",
    "\n",
    "**Best for:** research papers, narrative documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Rule of Thumb\n",
    "\n",
    "**Start with 400-500 tokens (roughly 300-400 words)**\n",
    "\n",
    "This is a good default for most use cases. You can adjust based on:\n",
    "- Document type (technical docs â†’ smaller, narratives â†’ larger)\n",
    "- Query types (specific facts â†’ smaller, summaries â†’ larger)\n",
    "- Testing and evaluation results\n",
    "\n",
    "**Remember:** There's no perfect size - it depends on your use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Chunking Strategies\n",
    "\n",
    "Let's build chunking strategies from scratch to understand how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Sample Document\n",
    "\n",
    "First, let's create a sample document to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document about Python programming\n",
    "sample_document = \"\"\"\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
    "It was created by Guido van Rossum and first released in 1991. Python supports multiple programming \n",
    "paradigms including procedural, object-oriented, and functional programming.\n",
    "\n",
    "One of Python's key strengths is its extensive standard library, which provides tools for many common \n",
    "programming tasks. The language emphasizes code readability with its use of significant indentation. \n",
    "Python's syntax allows programmers to express concepts in fewer lines of code compared to languages \n",
    "like C++ or Java.\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, scientific computing, \n",
    "and automation. Popular frameworks include Django and Flask for web development, NumPy and Pandas for \n",
    "data analysis, and TensorFlow and PyTorch for machine learning.\n",
    "\n",
    "The Python Package Index (PyPI) hosts hundreds of thousands of third-party packages that extend Python's \n",
    "capabilities. Installation is simple using pip, Python's package installer. The active community contributes \n",
    "to a rich ecosystem of libraries and frameworks.\n",
    "\n",
    "Python continues to be one of the most popular programming languages worldwide. Its beginner-friendly nature \n",
    "makes it ideal for education, while its powerful features support professional software development and research.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(sample_document)} characters\")\n",
    "print(f\"Document length: {len(sample_document.split())} words\")\n",
    "print(f\"\\nFirst 200 characters:\\n{sample_document[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Fixed-Size Chunking (Character-Based)\n",
    "\n",
    "**Simplest approach:** Split text every N characters.\n",
    "\n",
    "**How it works:**\n",
    "1. Set a chunk size (e.g., 200 characters)\n",
    "2. Split the text at every 200 characters\n",
    "3. Optionally add overlap between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_characters(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into chunks of specified character length.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        chunk_size: Number of characters per chunk\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Get chunk from start to start + chunk_size\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_characters(sample_document, chunk_size=200, overlap=50)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):  # Show first 3 chunks\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Observations\n",
    "\n",
    "**Notice the problems with character-based chunking:**\n",
    "- Chunks split in the middle of words\n",
    "- Chunks split in the middle of sentences\n",
    "- No respect for natural boundaries\n",
    "\n",
    "**This is why we need smarter chunking strategies!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fixed-Size Chunking (Word-Based)\n",
    "\n",
    "**Better approach:** Split by words instead of characters.\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into words\n",
    "2. Group words into chunks of N words\n",
    "3. Join words back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_words(text, chunk_size=50, overlap=10):\n",
    "    \"\"\"\n",
    "    Split text into chunks of specified word count.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        chunk_size: Number of words per chunk\n",
    "        overlap: Number of words to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        # Get chunk of words\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        \n",
    "        # Join words back into text\n",
    "        chunk = ' '.join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_words(sample_document, chunk_size=50, overlap=10)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):  # Show first 3 chunks\n",
    "    print(f\"Chunk {i} ({len(chunk.split())} words):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Better, but still not perfect!\n",
    "\n",
    "**Improvements:**\n",
    "- No more mid-word splits âœ…\n",
    "- Easier to control chunk size âœ…\n",
    "\n",
    "**Remaining issues:**\n",
    "- Still splits in the middle of sentences âŒ\n",
    "- Doesn't respect paragraph boundaries âŒ\n",
    "\n",
    "**Let's fix this with sentence-based chunking!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sentence-Based Chunking\n",
    "\n",
    "**Better approach:** Split by sentences, then group into chunks.\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into sentences\n",
    "2. Group sentences until reaching target chunk size\n",
    "3. Preserves sentence boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(text, max_chunk_size=500):\n",
    "    \"\"\"\n",
    "    Split text into chunks by sentences, keeping sentences intact.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (split on . ! ?)\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Check if adding this sentence would exceed max size\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "            # Save current chunk and start new one\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_sentences(sample_document, max_chunk_size=400)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Much better!\n",
    "\n",
    "**Advantages:**\n",
    "- Complete sentences âœ…\n",
    "- More natural reading âœ…\n",
    "- Better semantic coherence âœ…\n",
    "\n",
    "**This is the recommended approach for most use cases!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Paragraph-Based Chunking\n",
    "\n",
    "**For structured documents:** Use natural paragraph breaks.\n",
    "\n",
    "**How it works:**\n",
    "1. Split by paragraph (double newlines)\n",
    "2. Each paragraph becomes a chunk (or combine small ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_paragraphs(text, min_chunk_size=100):\n",
    "    \"\"\"\n",
    "    Split text by paragraphs (double newlines).\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        min_chunk_size: Minimum characters per chunk (combine small paragraphs)\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split by double newlines (paragraph separator)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "            \n",
    "        # If paragraph is too small, combine with next\n",
    "        if len(para) < min_chunk_size:\n",
    "            current_chunk += \"\\n\\n\" + para if current_chunk else para\n",
    "        else:\n",
    "            # Save previous chunk if exists\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            # Start new chunk with this paragraph\n",
    "            current_chunk = para\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_paragraphs(sample_document, min_chunk_size=100)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use paragraph chunking:\n",
    "\n",
    "**Good for:**\n",
    "- Well-structured documents (articles, reports, documentation)\n",
    "- Documents where paragraphs are coherent units\n",
    "- Markdown or formatted text\n",
    "\n",
    "**Not good for:**\n",
    "- Unstructured text (chat logs, transcripts)\n",
    "- Very long paragraphs\n",
    "- Documents without clear paragraph breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Chunk Overlap: Why It Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 The Problem with No Overlap\n",
    "\n",
    "**Consider this example:**\n",
    "\n",
    "```\n",
    "Chunk 1: \"...the contract term is 12 months.\"\n",
    "Chunk 2: \"The renewal process requires 30 days notice...\"\n",
    "```\n",
    "\n",
    "**Query:** \"What is the contract term and renewal notice period?\"\n",
    "\n",
    "**Problem:** The answer is split across two chunks with no connection!\n",
    "\n",
    "## 4.2 The Solution: Overlap\n",
    "\n",
    "**With overlap:**\n",
    "```\n",
    "Chunk 1: \"...the contract term is 12 months. The renewal process...\"\n",
    "Chunk 2: \"...12 months. The renewal process requires 30 days notice...\"\n",
    "```\n",
    "\n",
    "Now Chunk 2 has context from Chunk 1!\n",
    "\n",
    "## 4.3 How Much Overlap?\n",
    "\n",
    "**Rule of thumb:** 10-20% of chunk size\n",
    "\n",
    "- Chunk size 500 words â†’ Overlap 50-100 words\n",
    "- Chunk size 1000 tokens â†’ Overlap 100-200 tokens\n",
    "\n",
    "**Trade-off:**\n",
    "- More overlap = Better context, but more storage and processing\n",
    "- Less overlap = Less storage, but may lose connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Adding Metadata to Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Why Metadata Matters\n",
    "\n",
    "**Metadata = Information about the chunk**\n",
    "\n",
    "**Useful metadata:**\n",
    "- Source document name\n",
    "- Page number\n",
    "- Chunk index\n",
    "- Section/chapter title\n",
    "- Creation date\n",
    "- Author\n",
    "- Document type\n",
    "\n",
    "**Why it's important:**\n",
    "- **Source attribution:** Tell users where information came from\n",
    "- **Filtering:** Retrieve only from specific sources or dates\n",
    "- **Navigation:** Link back to original document\n",
    "- **Verification:** Users can verify information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Implementing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_metadata(text, source_name, chunk_size=50, overlap=10):\n",
    "    \"\"\"\n",
    "    Create chunks with metadata.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        source_name: Name of the source document\n",
    "        chunk_size: Number of words per chunk\n",
    "        overlap: Number of words to overlap\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with 'text' and 'metadata'\n",
    "    \"\"\"\n",
    "    # Get basic chunks\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = ' '.join(chunk_words)\n",
    "        \n",
    "        # Create chunk with metadata\n",
    "        chunk_with_meta = {\n",
    "            'text': chunk_text,\n",
    "            'metadata': {\n",
    "                'source': source_name,\n",
    "                'chunk_index': chunk_index,\n",
    "                'chunk_size': len(chunk_words),\n",
    "                'char_count': len(chunk_text),\n",
    "                'start_word': start,\n",
    "                'end_word': end\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        chunks.append(chunk_with_meta)\n",
    "        start += chunk_size - overlap\n",
    "        chunk_index += 1\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_with_metadata(\n",
    "    sample_document, \n",
    "    source_name=\"python_intro.txt\",\n",
    "    chunk_size=50,\n",
    "    overlap=10\n",
    ")\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\\n\")\n",
    "print(\"First chunk with metadata:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Text: {chunks[1]['text'][:200]}...\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in chunks[1]['metadata'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Practical Example: Loading and Chunking a Real Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Loading Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk_text_file(file_path, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Load a text file and chunk it.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the text file\n",
    "        chunk_size: Characters per chunk\n",
    "        overlap: Character overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of chunks with metadata\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Get file metadata\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    \n",
    "    # Chunk the text\n",
    "    chunks = chunk_by_sentences(text, max_chunk_size=chunk_size)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    chunks_with_metadata = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunks_with_metadata.append({\n",
    "            'text': chunk,\n",
    "            'metadata': {\n",
    "                'source': file_name,\n",
    "                'file_path': file_path,\n",
    "                'file_size': file_size,\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return chunks_with_metadata\n",
    "\n",
    "# Example usage (create a sample file first)\n",
    "sample_file_path = 'sample_document.txt'\n",
    "with open(sample_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_document)\n",
    "\n",
    "# Load and chunk\n",
    "chunks = load_and_chunk_text_file(sample_file_path, chunk_size=400)\n",
    "\n",
    "print(f\"Loaded and chunked: {chunks[0]['metadata']['source']}\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"\\nChunk 1:\")\n",
    "print(chunks[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[2]['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Loading PDFs (Using PyPDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPDF2 if needed\n",
    "!pip install -q PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk_pdf(file_path, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Load a PDF file and chunk it.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PDF file\n",
    "        chunk_size: Characters per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of chunks with metadata (including page numbers)\n",
    "    \"\"\"\n",
    "    import PyPDF2\n",
    "    import os\n",
    "    \n",
    "    chunks_with_metadata = []\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Open PDF\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num in range(num_pages):\n",
    "            # Extract text from page\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            \n",
    "            # Chunk the page text\n",
    "            page_chunks = chunk_by_sentences(text, max_chunk_size=chunk_size)\n",
    "            \n",
    "            # Add metadata to each chunk\n",
    "            for chunk_idx, chunk in enumerate(page_chunks):\n",
    "                chunks_with_metadata.append({\n",
    "                    'text': chunk,\n",
    "                    'metadata': {\n",
    "                        'source': file_name,\n",
    "                        'page': page_num + 1,  # 1-indexed\n",
    "                        'total_pages': num_pages,\n",
    "                        'chunk_on_page': chunk_idx,\n",
    "                    }\n",
    "                })\n",
    "    \n",
    "    return chunks_with_metadata\n",
    "\n",
    "# Example (you would use this with a real PDF file)\n",
    "print(\"PDF loading function ready!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"chunks = load_and_chunk_pdf('your_document.pdf', chunk_size=500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test the load_and_chunk function with the code below\n",
    "\n",
    "# chunks = load_and_chunk_pdf(\"Retrieval-Augmented_Generation_RAG.pdf\", chunk_size=500)\n",
    "\n",
    "# print(f\"Total chunks: {len(chunks)}\\n\")\n",
    "\n",
    "# for i, c in enumerate(chunks[:5]):  # show first 5 chunks\n",
    "#     print(f\"CHUNK {i+1}\")\n",
    "#     print(\"TEXT:\", c[\"text\"][:200])  # print first 200 chars\n",
    "#     print(\"META:\", c[\"metadata\"])\n",
    "#     print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Choosing the Right Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Start with sentence-based chunking (400-500 chars)** - works for 80% of cases\n",
    "2. **Use paragraph-based for well-structured documents**\n",
    "3. **Increase chunk size** if answers need more context\n",
    "4. **Decrease chunk size** if you need more precise retrieval\n",
    "5. **Always use some overlap** (10-20%) to maintain context\n",
    "6. **Test and iterate** - there's no perfect size for all cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Chunking is essential** for RAG - whole documents don't work well\n",
    "\n",
    "2. **Chunk size matters:**\n",
    "   - Small chunks (100-300 tokens): Precise but may lose context\n",
    "   - Medium chunks (300-600 tokens): Best starting point\n",
    "   - Large chunks (600-1000 tokens): More context but less precise\n",
    "\n",
    "3. **Chunking strategies:**\n",
    "   - Character-based: Simple but breaks words/sentences\n",
    "   - Word-based: Better, but still breaks sentences\n",
    "   - Sentence-based: **Recommended for most cases**\n",
    "   - Paragraph-based: Good for structured documents\n",
    "\n",
    "4. **Overlap is important:**\n",
    "   - Maintains context between chunks\n",
    "   - 10-20% overlap is typical\n",
    "\n",
    "5. **Metadata is crucial:**\n",
    "   - Enables source attribution\n",
    "   - Allows filtering and navigation\n",
    "   - Builds trust and verifiability\n",
    "\n",
    "6. **No perfect solution:**\n",
    "   - Different documents need different strategies\n",
    "   - Test and iterate to find what works\n",
    "   - Start simple, then optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Practice Exercises\n",
    "\n",
    "## Exercise 1: Chunking Different Document Types\n",
    "\n",
    "### Task\n",
    "Apply appropriate chunking strategies to different document types.\n",
    "\n",
    "### Documents\n",
    "\n",
    "You'll chunk 3 different types of documents:\n",
    "\n",
    "**Document A: FAQ**\n",
    "```\n",
    "Q: What is the return policy?\n",
    "A: Items can be returned within 30 days of purchase with original receipt.\n",
    "\n",
    "Q: Do you offer international shipping?\n",
    "A: Yes, we ship to over 50 countries worldwide. Shipping times vary by location.\n",
    "\n",
    "Q: How do I track my order?\n",
    "A: Use the tracking number sent to your email after shipment.\n",
    "```\n",
    "\n",
    "**Document B: Technical Documentation**\n",
    "```\n",
    "Installation Guide\n",
    "\n",
    "Step 1: Download the installer from our website.\n",
    "Extract the zip file to your desired location.\n",
    "\n",
    "Step 2: Run setup.exe as administrator.\n",
    "Follow the on-screen instructions.\n",
    "\n",
    "Step 3: Configure your API key in the settings file.\n",
    "The settings file is located at config/settings.json.\n",
    "```\n",
    "\n",
    "**Document C: Article**\n",
    "```\n",
    "The Future of Renewable Energy\n",
    "\n",
    "Solar and wind power have seen tremendous growth in recent years. As technology improves\n",
    "and costs decrease, renewable energy becomes increasingly competitive with fossil fuels.\n",
    "\n",
    "Energy storage solutions are critical for renewable adoption. Battery technology advances\n",
    "enable better grid management and reliability. This addresses the intermittent nature of\n",
    "solar and wind power.\n",
    "\n",
    "Policy support and public awareness continue to drive the transition. Many countries have\n",
    "set ambitious renewable energy targets for the coming decades.\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "For each document:\n",
    "1. Choose the most appropriate chunking strategy (character, word, sentence, paragraph, or custom)\n",
    "2. Explain why you chose that strategy\n",
    "3. Implement the chunking\n",
    "4. Show the resulting chunks\n",
    "\n",
    "### Template\n",
    "\n",
    "```python\n",
    "# Document A: FAQ\n",
    "strategy_A = \"?\"  # Your choice\n",
    "reason_A = \"?\"    # Why this strategy?\n",
    "chunks_A = ?      # Your implementation\n",
    "\n",
    "# Document B: Technical Documentation\n",
    "strategy_B = \"?\"\n",
    "reason_B = \"?\"\n",
    "chunks_B = ?\n",
    "\n",
    "# Document C: Article\n",
    "strategy_C = \"?\"\n",
    "reason_C = \"?\"\n",
    "chunks_C = ?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "In **Module 3: Embeddings Deep Dive**, you'll learn:\n",
    "- How embedding models work\n",
    "- Generating embeddings for your chunks\n",
    "- Choosing the right embedding model\n",
    "- Understanding vector similarity\n",
    "\n",
    "Get ready to turn your text chunks into vectors! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "publica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
