{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Technique 2: Contextual Compression\n",
        "\n",
        "## The Problem\n",
        "Basic RAG retrieves entire document chunks (often 500-1000 tokens each). Most of this content is **irrelevant** to the query. You're:\n",
        "- Wasting tokens (cost)\n",
        "- Adding noise (confusion)\n",
        "- Hitting context limits\n",
        "\n",
        "## The Solution\n",
        "Use an LLM to **compress** retrieved chunks, keeping only information relevant to the query.\n",
        "\n",
        "**Difficulty:** ⭐⭐⭐☆☆"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils_openai import setup_openai_api, create_embeddings, create_llm, load_msme_data, create_vectorstore, get_baseline_prompt, count_tokens_approximate\n",
        "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "print('[OK] Imports done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = setup_openai_api()\n",
        "embeddings = create_embeddings(api_key)\n",
        "llm = create_llm(api_key)\n",
        "docs, metas, ids = load_msme_data('msme.csv')\n",
        "vectorstore = create_vectorstore(docs, metas, ids, embeddings, 'msme_t3', './chroma_db_t3')\n",
        "base_retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
        "print('[OK] Base retriever ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Compressor\n",
        "The LLM will filter each retrieved chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "print('[OK] Compressor created!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Wrap with ContextualCompressionRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=base_retriever\n",
        ")\n",
        "print('[OK] Compression retriever ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = get_baseline_prompt()\n",
        "\n",
        "compression_rag_chain = (\n",
        "    {'context': compression_retriever, 'question': RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "print('[OK] Compression RAG chain ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test and Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'How do I register a construction business?'\n",
        "\n",
        "# Get baseline (uncompressed) docs\n",
        "baseline_docs = base_retriever.invoke(question)\n",
        "baseline_text = '\\n\\n'.join([d.page_content for d in baseline_docs])\n",
        "baseline_tokens = count_tokens_approximate(baseline_text)\n",
        "\n",
        "# Get compressed docs\n",
        "compressed_docs = compression_retriever.invoke(question)\n",
        "compressed_text = '\\n\\n'.join([d.page_content for d in compressed_docs])\n",
        "compressed_tokens = count_tokens_approximate(compressed_text)\n",
        "\n",
        "print(f'BASELINE: {len(baseline_docs)} docs, ~{baseline_tokens} tokens')\n",
        "print(f'COMPRESSED: {len(compressed_docs)} docs, ~{compressed_tokens} tokens')\n",
        "print(f'REDUCTION: {((baseline_tokens-compressed_tokens)/baseline_tokens*100):.1f}%')\n",
        "\n",
        "# Get answer\n",
        "answer = compression_rag_chain.invoke(question)\n",
        "print(f'\\nANSWER:\\n{answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use\n",
        "**Use when:**\n",
        "- Long retrieved documents\n",
        "- High token costs\n",
        "- Need focused context\n",
        "\n",
        "**Avoid when:**\n",
        "- Documents already short\n",
        "- Extra LLM call unacceptable\n",
        "- Need full context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "1. Compare token usage before/after compression\n",
        "2. Test with different queries\n",
        "3. Check if quality improves or degrades\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Next:** Technique 3 - Semantic Chunking"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bao_env (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
