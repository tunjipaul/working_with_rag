{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2: Tool Integration for Agents\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what tools are and why agents need them\n",
    "- Create tools using the `@tool` decorator\n",
    "- Bind tools to LLMs for function calling\n",
    "- Implement conditional routing based on tool calls\n",
    "- Build an agent that decides which tools to use\n",
    "\n",
    "**Prerequisites:** Topic 1 (LangGraph Basics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Why Do Agents Need Tools?\n",
    "\n",
    "### The Problem: LLMs Can't Do Everything\n",
    "\n",
    "LLMs are great at:\n",
    "- ‚úÖ Understanding language\n",
    "- ‚úÖ Generating text\n",
    "- ‚úÖ Reasoning about information\n",
    "\n",
    "But they **can't**:\n",
    "- ‚ùå Search the web in real-time\n",
    "- ‚ùå Perform precise calculations\n",
    "- ‚ùå Access databases or files\n",
    "- ‚ùå Call APIs\n",
    "- ‚ùå Execute code\n",
    "\n",
    "### The Solution: Tools!\n",
    "\n",
    "**Tools** are functions that agents can call to perform actions:\n",
    "\n",
    "```\n",
    "User: \"What's 12345 * 67890?\"\n",
    "\n",
    "Agent thinks: \"I need to calculate this precisely\"\n",
    "         ‚Üì\n",
    "Agent calls: calculator_tool(\"12345 * 67890\")\n",
    "         ‚Üì\n",
    "Tool returns: \"838102050\"\n",
    "         ‚Üì\n",
    "Agent responds: \"The answer is 838,102,050\"\n",
    "```\n",
    "\n",
    "This is the foundation of **agentic behavior** - agents that can DO things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Reflection Question:** \n",
    "How is this different from just calling functions in your code? The key is that the **agent decides** when to call the tool - you don't hardcode it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"‚úÖ API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,  # Lower temperature for more precise tool usage\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Creating Your First Tool\n",
    "\n",
    "Let's start with a simple calculator tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The @tool Decorator\n",
    "\n",
    "The `@tool` decorator converts a Python function into a tool that LLMs can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate a mathematical expression and return the result.\n",
    "    Use this tool when you need to perform calculations.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression like \"2 + 2\" or \"15 * 37\"\n",
    "        \n",
    "    Returns:\n",
    "        The calculated result as a string\n",
    "        \n",
    "    Examples:\n",
    "        - \"2 + 2\" returns \"4\"\n",
    "        - \"100 / 5\" returns \"20.0\"\n",
    "        - \"2 ** 10\" returns \"1024\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Evaluate the expression safely\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Calculator tool created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Components of a Good Tool:**\n",
    "\n",
    "1. **Clear docstring** - LLM reads this to understand when to use the tool!\n",
    "2. **Type hints** - Helps LLM know what arguments to provide\n",
    "3. **Examples** - Shows LLM how to use the tool\n",
    "4. **Error handling** - Gracefully handle failures\n",
    "5. **Return string** - LLMs work best with string outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Tool Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the calculator tool\n",
    "result = calculator.invoke({\"expression\": \"123 * 456\"})\n",
    "print(f\"123 * 456 = {result}\")\n",
    "\n",
    "result2 = calculator.invoke(\"2 ** 10\")\n",
    "print(f\"2^10 = {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Creating a Second Tool\n",
    "\n",
    "Agents are more useful with multiple tools! Let's add a string manipulation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def text_analyzer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze text and return statistics about it.\n",
    "    Use this tool when you need to analyze or count things in text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Statistics about the text (characters, words, sentences)\n",
    "        \n",
    "    Examples:\n",
    "        - \"Hello world\" returns character count, word count, etc.\n",
    "    \"\"\"\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
    "    \n",
    "    return f\"\"\"Text Analysis:\n",
    "- Characters: {char_count}\n",
    "- Words: {word_count}\n",
    "- Sentences: {sentence_count}\n",
    "- First 50 chars: {text[:50]}...\"\"\"\n",
    "\n",
    "print(\"‚úÖ Text analyzer tool created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the text analyzer\n",
    "test_text = \"Hello! This is a test. How are you today?\"\n",
    "result = text_analyzer.invoke({\"text\": test_text})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Binding Tools to the LLM\n",
    "\n",
    "Now we need to tell the LLM about our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tools\n",
    "tools = [calculator, text_analyzer]\n",
    "\n",
    "# Bind tools to the LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(f\"‚úÖ LLM bound to {len(tools)} tools\")\n",
    "print(f\"   Tools: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What `bind_tools` does:**\n",
    "1. Sends tool descriptions to the LLM\n",
    "2. LLM can now \"see\" what tools are available\n",
    "3. LLM will decide when to call tools based on user queries\n",
    "\n",
    "This uses **OpenAI's function calling** feature - the LLM returns structured tool calls!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: LLM Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Does LLM decide to call calculator?\n",
    "response = llm_with_tools.invoke([HumanMessage(content=\"What is 234 * 567?\")])\n",
    "\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"\\nContent: {response.content}\")\n",
    "print(f\"\\nTool calls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Key Observation:** \n",
    "The LLM didn't return a direct answer - it returned a **tool call**! This is the agent saying \"I need to use the calculator tool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Does LLM decide NOT to call tools for simple queries?\n",
    "response2 = llm_with_tools.invoke([HumanMessage(content=\"Hello! How are you?\")])\n",
    "\n",
    "print(f\"Content: {response2.content}\")\n",
    "print(f\"Tool calls: {response2.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Smart Decision:** The LLM knows it doesn't need tools for greetings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Building the Agent Graph\n",
    "\n",
    "Now let's build a complete agent that can use these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Assistant Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt that encourages tool usage\n",
    "sys_msg = SystemMessage(content=\"\"\"You are a helpful assistant with access to tools.\n",
    "\n",
    "When asked to perform calculations, use the calculator tool.\n",
    "When asked to analyze text, use the text_analyzer tool.\n",
    "\n",
    "Only use tools when necessary - for simple questions, answer directly.\"\"\")\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    Assistant node - decides whether to use tools or answer directly.\n",
    "    \"\"\"\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "print(\"‚úÖ Assistant node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Conditional Routing\n",
    "\n",
    "This is the **key to agentic behavior** - the graph decides where to go based on whether tools were called!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Decide next step based on last message.\n",
    "    \n",
    "    If LLM called a tool ‚Üí go to 'tools' node\n",
    "    If LLM provided final answer ‚Üí go to END\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Check if LLM made tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # No tool calls - we're done\n",
    "    return \"__end__\"\n",
    "\n",
    "print(\"‚úÖ Conditional routing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Understanding the Flow:**\n",
    "\n",
    "```\n",
    "User Query ‚Üí Assistant Node ‚Üí Tool calls?\n",
    "                                  ‚îú‚îÄ YES ‚Üí Tools Node ‚Üí Back to Assistant\n",
    "                                  ‚îî‚îÄ NO  ‚Üí END (return answer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))  # ToolNode executes tool calls automatically\n",
    "\n",
    "# Define edges\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    should_continue,\n",
    "    {\"tools\": \"tools\", \"__end__\": END}\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")  # After tools, go back to assistant\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Agent graph compiled with tools and memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Note on ToolNode:**\n",
    "`ToolNode(tools)` is a LangGraph helper that:\n",
    "1. Reads tool calls from the last message\n",
    "2. Executes the appropriate tool\n",
    "3. Returns results as ToolMessage\n",
    "\n",
    "This saves us from manually implementing tool execution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the agent graph\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph structure: START ‚Üí assistant ‚Üí [conditional] ‚Üí tools ‚Üí assistant ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üé® Notice the difference from Topic 1:**\n",
    "- Topic 1: Linear (START ‚Üí assistant ‚Üí END)\n",
    "- Topic 2: Has a **cycle** (tools can loop back to assistant)\n",
    "\n",
    "This is agentic behavior - the agent can use tools multiple times if needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Testing the Tool-Using Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def run_agent(user_input: str, thread_id: str = \"test_session\"):\n",
    "    \"\"\"\n",
    "    Run the agent and display the conversation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üë§ User: {user_input}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            continue  # Already printed\n",
    "        elif isinstance(message, AIMessage):\n",
    "            if message.tool_calls:\n",
    "                print(f\"ü§ñ Agent: [Calling tool: {message.tool_calls[0]['name']}]\")\n",
    "            else:\n",
    "                print(f\"ü§ñ Agent: {message.content}\")\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"üîß Tool Result: {message.content[:100]}...\" if len(message.content) > 100 else f\"üîß Tool Result: {message.content}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "print(\"‚úÖ Test function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Calculator Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"What is 12345 * 67890?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Expected Flow:**\n",
    "1. Agent sees it needs to calculate\n",
    "2. Calls calculator tool\n",
    "3. Gets result from tool\n",
    "4. Returns formatted answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Text Analyzer Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"Analyze this text: 'RAG systems combine retrieval with generation. They are very useful!'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: No Tool Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"Hello! What can you help me with?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Smart Agent:** Notice it didn't use any tools - it knew a greeting doesn't need tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Wrong Tool Choice?\n",
    "\n",
    "Let's see if the agent chooses the right tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"How many words are in this sentence: 'LangGraph makes building agents easy'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Expected:** Should use `text_analyzer`, not `calculator`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Conversational Context with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First query\n",
    "run_agent(\"Calculate 100 * 50\", thread_id=\"calc_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up - does it remember?\n",
    "run_agent(\"Now add 1000 to that result\", thread_id=\"calc_session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Amazing:** The agent remembers the previous result AND uses the calculator for the new calculation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Understanding Tool Messages\n",
    "\n",
    "Let's inspect what happens behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full message history\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is 15 * 25?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"inspect_session\"}}\n",
    ")\n",
    "\n",
    "print(\"\\nüìã FULL MESSAGE HISTORY:\\n\")\n",
    "for i, msg in enumerate(result[\"messages\"], 1):\n",
    "    print(f\"{i}. {type(msg).__name__}\")\n",
    "    if isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "        print(f\"   Tool Call: {msg.tool_calls[0]['name']}({msg.tool_calls[0]['args']})\")\n",
    "    elif isinstance(msg, ToolMessage):\n",
    "        print(f\"   Content: {msg.content}\")\n",
    "    elif hasattr(msg, 'content'):\n",
    "        print(f\"   Content: {msg.content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Message Flow:**\n",
    "1. `HumanMessage` - User's query\n",
    "2. `AIMessage` (with tool_calls) - Agent decides to call calculator\n",
    "3. `ToolMessage` - Result from calculator tool\n",
    "4. `AIMessage` (no tool_calls) - Agent's final answer\n",
    "\n",
    "This is the standard **ReAct** pattern: Reason ‚Üí Act ‚Üí Observe ‚Üí Respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Adding a Third Tool (Bonus)\n",
    "\n",
    "Let's add one more tool to show how flexible this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def coin_flip() -> str:\n",
    "    \"\"\"\n",
    "    Flip a coin and return heads or tails.\n",
    "    \n",
    "    Use this when the user wants a random choice or coin flip.\n",
    "    \n",
    "    Returns:\n",
    "        Either \"Heads\" or \"Tails\"\n",
    "    \"\"\"\n",
    "    import random\n",
    "    return random.choice([\"Heads\", \"Tails\"])\n",
    "\n",
    "# Rebuild agent with 3 tools\n",
    "tools_v2 = [calculator, text_analyzer, coin_flip]\n",
    "llm_with_tools_v2 = llm.bind_tools(tools_v2)\n",
    "\n",
    "def assistant_v2(state: MessagesState) -> dict:\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    response = llm_with_tools_v2.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "builder_v2 = StateGraph(MessagesState)\n",
    "builder_v2.add_node(\"assistant\", assistant_v2)\n",
    "builder_v2.add_node(\"tools\", ToolNode(tools_v2))\n",
    "builder_v2.add_edge(START, \"assistant\")\n",
    "builder_v2.add_conditional_edges(\"assistant\", should_continue, {\"tools\": \"tools\", \"__end__\": END})\n",
    "builder_v2.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "agent_v2 = builder_v2.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"‚úÖ Agent v2 created with 3 tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test coin flip\n",
    "result = agent_v2.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Flip a coin for me!\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"coin_session\"}}\n",
    ")\n",
    "\n",
    "for msg in result[\"messages\"]:\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"ü§ñ Agent: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ú® Scalability:** You can add as many tools as you need - the agent will learn to use them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Preparing for Topic 3 (Agentic RAG)\n",
    "\n",
    "In Topic 3, we'll create a **retrieval tool** that searches a vector store. It will work exactly like these tools:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents from vector store.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query)\n",
    "    return format_docs(docs)\n",
    "\n",
    "# Agent will decide: \"Do I need to retrieve documents?\"\n",
    "```\n",
    "\n",
    "This is the foundation of **Agentic RAG** - retrieval controlled by an intelligent agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Why Tools Matter**\n",
    "   - LLMs can't DO things without tools\n",
    "   - Tools extend agent capabilities\n",
    "   - Agents decide when to use tools\n",
    "\n",
    "2. **Creating Tools**\n",
    "   - Use `@tool` decorator\n",
    "   - Write clear docstrings (LLM reads them!)\n",
    "   - Include examples and error handling\n",
    "\n",
    "3. **Tool Integration**\n",
    "   - `bind_tools()` gives LLM awareness of tools\n",
    "   - OpenAI function calling enables structured tool calls\n",
    "   - ToolNode executes tools automatically\n",
    "\n",
    "4. **Conditional Routing**\n",
    "   - Check `tool_calls` to decide next step\n",
    "   - Graph can loop back to assistant after tools\n",
    "   - This enables iterative, multi-step reasoning\n",
    "\n",
    "5. **ReAct Pattern**\n",
    "   - **Reason:** Agent analyzes query\n",
    "   - **Act:** Agent calls appropriate tool\n",
    "   - **Observe:** Agent sees tool result\n",
    "   - **Respond:** Agent generates final answer\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Topic 3: Agentic RAG** ‚≠ê\n",
    "- Create a retrieval tool using Chroma vector store\n",
    "- Agent decides when to retrieve vs answer from knowledge\n",
    "- Build a complete agentic RAG system\n",
    "- The core concept of this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Other Practice Exercises\n",
    "\n",
    "1. **Create a new tool** that converts temperatures (Celsius ‚Üî Fahrenheit)\n",
    "2. **Test tool priority** - what happens if multiple tools could work?\n",
    "3. **Add error handling** - make a tool that sometimes fails and see how the agent handles it\n",
    "4. **Multi-tool query** - ask something that requires using TWO tools in sequence\n",
    "5. **Improve prompts** - modify the system prompt to change tool usage behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "## Exercise 1: Build Your First Stateful Agent\n",
    "\n",
    "### Task\n",
    "Create an agent with three custom tools:\n",
    "1. **Weather tool:** Returns simulated weather for a given city\n",
    "2. **Dictionary tool:** Looks up word definitions (simulate with a small dict)\n",
    "3. **Web search tool:** Uses DuckDuckGo to search the web for information\n",
    "\n",
    "### Requirements\n",
    "1. Define tools using `@tool` decorator\n",
    "2. Bind tools to LLM\n",
    "3. Implement conditional routing (agent decides which tool to use)\n",
    "4. Handle cases where no tool is needed\n",
    "5. Install DuckDuckGo search: `pip install duckduckgo-search`\n",
    "6. Use `DDGS().text()` method for web searches\n",
    "\n",
    "### Example Queries\n",
    "- \"What's the weather in Lagos?\" ‚Üí Uses weather tool\n",
    "- \"Define the word 'ephemeral'\" ‚Üí Uses dictionary\n",
    "- \"Search for latest AI news\" ‚Üí Uses DuckDuckGo web search\n",
    "- \"What's the capital of France?\" ‚Üí No tool needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "1. **How does the agent \"know\" which tool to use?**\n",
    "   \n",
    "2. **What role does the tool docstring play?**\n",
    "   \n",
    "3. **Why do we need conditional edges for tool-using agents?**\n",
    "   \n",
    "4. **What would happen if we didn't loop back to assistant after tools?**\n",
    "   \n",
    "5. **How is this different from just calling functions in regular Python code?**\n",
    "\n",
    "Think about these before moving to Topic 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**üéâ Topic 2 Complete!**\n",
    "\n",
    "You now know how to build agents that use tools! Next: **Agentic RAG** - where retrieval becomes a tool that agents control!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
