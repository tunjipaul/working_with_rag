{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to RAG\n",
    " \n",
    "**Level:** Beginner  \n",
    "**Prerequisites:** Understanding of LLMs and Prompt Engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "- Explain the key limitations of standalone LLMs\n",
    "- Define Retrieval-Augmented Generation (RAG) and its core concept\n",
    "- Identify use cases where RAG is the appropriate solution\n",
    "- Distinguish between RAG, fine-tuning, and prompt engineering\n",
    "- Describe the high-level RAG pipeline (Indexing, Retrieval, Generation)\n",
    "- Understand the value proposition of RAG systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. LLM Limitations Review\n",
    "\n",
    "Before we understand why RAG is important, let's review the key limitations of Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Knowledge Cutoff Dates\n",
    "\n",
    "**Problem:** LLMs are trained on data up to a specific date and have no knowledge of events after that.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"Who did INEC declare as the winner of the Edo 2024 governorship election?\"\n",
    "LLM: \"I don't have information about events after my knowledge cutoff in April 2023.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hallucinations and Factual Accuracy\n",
    "\n",
    "**Problem:** LLMs can confidently generate false or invented information.\n",
    "\n",
    "**Why This Happens:**\n",
    "- LLMs predict probable text, not factual text\n",
    "- No mechanism to verify facts\n",
    "- Can generate plausible but incorrect information\n",
    "\n",
    "**Consequences:** Legal liability, medical risks, business decisions based on false data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Domain-Specific Knowledge Gaps & Private Data\n",
    "\n",
    "**Problem:** LLMs lack deep expertise and cannot access private data.\n",
    "\n",
    "**Examples:**\n",
    "- **Company-specific:** Internal processes, policies, product details\n",
    "- **Proprietary:** Trade secrets, unpublished research\n",
    "- **Private:** Customer data, confidential documents\n",
    "\n",
    "**Privacy Concerns:** Can't send proprietary data to LLM training, need controlled access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Dynamic and Real-Time Information\n",
    "\n",
    "**Problem:** Information changes constantly, but LLM knowledge is static.\n",
    "\n",
    "**Examples:** Stock prices, weather, inventory, breaking news, account status\n",
    "\n",
    "**Why This Matters:** Many applications require up-to-the-minute information, but retraining LLMs constantly is impractical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” Reflection Question\n",
    "\n",
    "**Think about it:** In the applications, chatbots, or tools youâ€™ve built using LLMs, can you identify situations where the model might fail or give unreliable results? Why do you think this happens, and how could it affect the users of your system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Introduction to RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What is RAG?\n",
    "\n",
    "> **Definition:**  \n",
    "> Retrieval-Augmented Generation (RAG) is a technique that enhances LLM responses by retrieving relevant information from external knowledge sources and including it in the prompt context.\n",
    "\n",
    "**The Core Idea:**\n",
    "\n",
    "Instead of relying solely on the LLM's parametric knowledge (what it learned during training), we:\n",
    "1. **Retrieve** relevant information from external sources\n",
    "2. **Augment** the prompt with this retrieved information\n",
    "3. **Generate** a response based on both the query and the retrieved context\n",
    "\n",
    "**Simple Analogy:**\n",
    "- **LLM without RAG:** Closed-book exam (only what you memorized)\n",
    "- **LLM with RAG:** Open-book exam (can reference materials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 How RAG Addresses LLM Limitations\n",
    "\n",
    "| LLM Limitation | How RAG Helps |\n",
    "|---------------|---------------|\n",
    "| **Knowledge cutoff** | Retrieve latest information from updated knowledge base |\n",
    "| **Hallucinations** | Ground responses in retrieved factual documents |\n",
    "| **Domain gaps** | Access specialized knowledge bases and documents |\n",
    "| **Private data** | Retrieve from private document collections securely |\n",
    "| **Dynamic info** | Query real-time databases and live data sources |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 RAG vs Alternatives: Quick Decision Guide\n",
    "\n",
    "**Use Prompt Engineering when:**\n",
    "- Task requires reasoning, formatting, or instruction-following\n",
    "- No external knowledge needed\n",
    "- Examples can fit in the prompt\n",
    "\n",
    "**Use Fine-tuning when:**\n",
    "- Need to change model behavior or style\n",
    "- Teaching new tasks or formats\n",
    "- Adapting to domain-specific language patterns\n",
    "- Knowledge is relatively static\n",
    "\n",
    "**Use RAG when:**\n",
    "- Need access to external knowledge\n",
    "- Information changes frequently\n",
    "- Large knowledge base (can't fit in prompt or fine-tuning)\n",
    "- Want to cite sources\n",
    "- Need to update knowledge without retraining\n",
    "\n",
    "**Use Combination (Fine-tuning + RAG) when:**\n",
    "- Need domain adaptation AND external knowledge\n",
    "- Example: Medical chatbot with hospital-specific knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Decision Matrix\n",
    "\n",
    "| Your Need | Best Approach |\n",
    "|-----------|---------------|\n",
    "| Access external/private documents | **RAG** |\n",
    "| Change model behavior or style | **Fine-tuning** |\n",
    "| Improve task performance with examples | **Prompt Engineering** (few-shot) |\n",
    "| Keep knowledge up-to-date | **RAG** |\n",
    "| Teach specific output format | **Fine-tuning** or **Prompt Engineering** |\n",
    "| Handle domain-specific language | **Fine-tuning** |\n",
    "| Cost-effective knowledge updates | **RAG** |\n",
    "| Reduce latency (no external calls) | **Fine-tuning** |\n",
    "| Provide source citations | **RAG** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Comparison Table\n",
    "\n",
    "| Dimension | Prompt Engineering | RAG | Fine-tuning |\n",
    "|-----------|-------------------|-----|-------------|\n",
    "| **Primary Use Case** | Improve task performance, provide examples, guide behavior | Access external knowledge, answer questions from documents | Teach new tasks, adapt to domain language, change behavior |\n",
    "| **Knowledge Update** | N/A (no knowledge added) | Easy: update documents in knowledge base | Hard: requires retraining |\n",
    "| **Cost** | $ (token usage only) | $$ (embeddings + vector DB + LLM) | $$$ (training compute + time) |\n",
    "| **Setup Time** | Minutes | Hours to days | Days to weeks |\n",
    "| **Technical Complexity** | Low | Medium | High |\n",
    "| **Latency** | Low (direct LLM call) | Medium (retrieval + LLM) | Low (direct LLM call) |\n",
    "| **Accuracy on Knowledge** | Low (depends on LLM memory) | High (grounded in documents) | Medium-High (learned during training) |\n",
    "| **Source Citations** | No | Yes (can cite retrieved docs) | No |\n",
    "| **Hallucination Risk** | High | Low (if retrieval succeeds) | Medium |\n",
    "| **When Knowledge Changes** | Re-prompt (easy) | Update docs (easy) | Retrain (expensive) |\n",
    "| **Data Requirements** | Few examples (0-50) | Document collection (any size) | Labeled training data (100s-1000s) |\n",
    "| **Best For** | Formatting, reasoning, few-shot learning | Q&A, document search, knowledge retrieval | Domain adaptation, style change, new tasks |\n",
    "| **Scalability** | High (stateless) | Medium (vector DB overhead) | High (once trained) |\n",
    "| **Customization** | Low (prompt-level only) | Medium (chunk strategy, retrieval) | High (model weights) |\n",
    "| **Maintenance** | Low | Medium (index updates) | High (periodic retraining) |\n",
    "| **Auditability** | High (see prompts) | High (see retrieved docs) | Low (black box weights) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. RAG Use Cases\n",
    "\n",
    "Let's explore real-world scenarios where RAG excels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common RAG Applications\n",
    "\n",
    "**1. Document Q&A Systems**\n",
    "- Search and answer questions from large document collections\n",
    "- Examples: Legal contracts, research papers, corporate policies\n",
    "- Benefits: Fast access, accurate citations, handles large collections\n",
    "\n",
    "**2. Customer Support**\n",
    "- Automated support using FAQs and documentation\n",
    "- Examples: E-commerce returns, troubleshooting\n",
    "- Benefits: 24/7 availability, consistent answers, reduced support volume\n",
    "\n",
    "**3. Knowledge Management**\n",
    "- Centralize internal company knowledge\n",
    "- Examples: Meeting notes, Slack messages, wikis, emails\n",
    "- Benefits: Faster onboarding, reduced search time, preserve institutional knowledge\n",
    "\n",
    "**4. Code & Technical Documentation**\n",
    "- Search codebases and API documentation\n",
    "- Examples: Function usage, integration guides, troubleshooting\n",
    "- Benefits: Faster development, better code reuse, improved onboarding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. The RAG Pipeline Overview\n",
    "\n",
    "RAG systems consist of two main phases: **Indexing** (offline) and **Query** (online)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Indexing Phase (Offline)\n",
    "\n",
    "**Goal:** Process documents and prepare them for efficient retrieval.\n",
    "\n",
    "### Step 1: Document Collection\n",
    "- Gather documents from various sources (PDFs, databases, web pages, etc.)\n",
    "- Clean and normalize data\n",
    "\n",
    "### Step 2: Document Chunking\n",
    "- Split documents into smaller, manageable pieces (chunks)\n",
    "- **Why?** Documents are usually too long to process at once\n",
    "- Typical chunk size: 200-1000 tokens\n",
    "- May include overlap between chunks to preserve context\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Original Document (5000 words)\n",
    "        â†“\n",
    "Chunking (500 words per chunk)\n",
    "        â†“\n",
    "10 chunks with 50-word overlap\n",
    "```\n",
    "\n",
    "### Step 3: Embedding Generation\n",
    "- Convert each chunk into a numerical vector (embedding)\n",
    "- Embeddings capture semantic meaning\n",
    "- Similar chunks have similar embeddings\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Text Chunk: \"The cat sat on the mat\"\n",
    "        â†“ (Embedding Model)\n",
    "Vector: [0.2, -0.5, 0.8, ..., 0.3] (e.g., 384 dimensions)\n",
    "\n",
    "Text Chunk 1: \"The cat sat on the mat\"\n",
    "Embedding: [0.2, -0.5, 0.8, ..., 0.3]\n",
    "\n",
    "Text Chunk 2: \"A cat was sitting on a rug\"\n",
    "Embedding: [0.21, -0.48, 0.79, ..., 0.31]\n",
    "\n",
    "```\n",
    "- Notice the vectors are very close â†’ RAG knows these chunks are semantically similar.\n",
    "\n",
    "### Step 4: Vector Storage\n",
    "- Store embeddings in a vector database\n",
    "- Store original text alongside embeddings\n",
    "- Store metadata (source, page number, date, etc.)\n",
    "\n",
    "**Vector Database Examples:**\n",
    "- FAISS (local, fast)\n",
    "- Pinecone (managed, cloud)\n",
    "- Chroma (embedded, easy)\n",
    "- Weaviate, Qdrant, Milvus (production-grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Query Phase (Online)\n",
    "\n",
    "**Goal:** Answer user queries using retrieved relevant information.\n",
    "\n",
    "### Step 1: Query Processing\n",
    "- Receive user question\n",
    "- Optionally preprocess (clean, reformulate, etc.)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User Query: \"What are the admission requirements for studying Engineering in Nigerian universities?\"\n",
    "```\n",
    "\n",
    "### Step 2: Query Embedding\n",
    "- Convert query to embedding using the **same model** as documents\n",
    "- This ensures query and document embeddings are in the same vector space\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"What are the admission requirements for studying Engineering in Nigerian universities?\"\n",
    "        â†“ (Embedding Model)\n",
    "Query Vector: [0.25, -0.48, 0.82, ..., 0.29]\n",
    "```\n",
    "\n",
    "### Step 3: Similarity Search\n",
    "- Find chunks with embeddings most similar to query embedding\n",
    "- Use cosine similarity or other distance metrics\n",
    "- Retrieve top-k most similar chunks (e.g., k=5)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query Vector compared against all document vectors\n",
    "        â†“\n",
    "Top 5 most similar chunks retrieved\n",
    "```\n",
    "\n",
    "### Step 4: Context Construction\n",
    "- Take retrieved chunks\n",
    "- Format them for inclusion in the prompt\n",
    "- Add metadata (sources, page numbers, etc.)\n",
    "\n",
    "**Example Prompt Structure:**\n",
    "```\n",
    "Context:\n",
    "[Chunk 1]: Engineering programs in Nigeria require five O'Level credits including Mathematics, Physics, Chemistry and English... \n",
    "           (Source: jamb_brochure.pdf, p.45)\n",
    "[Chunk 2]: Candidates must obtain a minimum JAMB score of 200 and pass the Post-UTME screening... \n",
    "           (Source: admission_guide.pdf, p.12)\n",
    "[Chunk 3]: Direct Entry candidates need at least two A'Level passes or equivalent in relevant science subjects... \n",
    "           (Source: university_handbook.pdf, p.8)\n",
    "\n",
    "Question: What are the admission requirements for studying Engineering in Nigerian universities?\n",
    "\n",
    "Answer:\n",
    "```\n",
    "\n",
    "### Step 5: LLM Generation\n",
    "- Send augmented prompt to LLM\n",
    "- LLM generates response based on:\n",
    "  - Retrieved context (primary source)\n",
    "  - Its own knowledge (secondary)\n",
    "  - Query understanding\n",
    "\n",
    "**Example Response:**\n",
    "```\n",
    "Based on the admission guidelines, the requirements for studying Engineering in Nigerian universities include:\n",
    "1. O'Level Requirements: Five credit passes including Mathematics, Physics, Chemistry, and English Language\n",
    "2. JAMB UTME: Minimum score of 200 in relevant subjects (Mathematics, Physics, Chemistry)\n",
    "3. Post-UTME: Successful completion of the university's screening examination\n",
    "4. Direct Entry: At least two A'Level passes or equivalent (NCE, OND) in relevant science subjects\n",
    "\n",
    "Sources: jamb_brochure.pdf (p.45), admission_guide.pdf (p.12), university_handbook.pdf (p.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Complete RAG Pipeline Diagram\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    INDEXING PHASE (Offline)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Documents â†’ Chunking â†’ Embeddings â†’ Vector Store\n",
    "                                          â†“\n",
    "                                    [Document Index]\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     QUERY PHASE (Online)                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "User Query â†’ Query Embedding â†’ Similarity Search\n",
    "                                      â†“\n",
    "                              Retrieve Top-K Chunks\n",
    "                                      â†“\n",
    "                            Construct Augmented Prompt\n",
    "                                      â†“\n",
    "                                   LLM Generation\n",
    "                                      â†“\n",
    "                                  Answer + Sources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Key Concepts to Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Embeddings\n",
    "\n",
    "**What are embeddings?**\n",
    "- Numerical representations of text (vectors of numbers)\n",
    "- Capture semantic meaning\n",
    "- Similar meanings â†’ Similar vectors\n",
    "\n",
    "**Why embeddings?**\n",
    "- Computers can't understand text directly\n",
    "- Embeddings enable mathematical comparison of meaning\n",
    "- Enable semantic search (search by meaning, not just keywords)\n",
    "\n",
    "**Example:**\n",
    "- \"The cat sat on the mat\" and \"A dog rested on the rug\" have **similar embeddings**\n",
    "- \"The cat sat on the mat\" and \"AI engineering is complex\" have **different embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Vector Similarity\n",
    "\n",
    "**How do we measure similarity?**\n",
    "\n",
    "**Cosine Similarity**:\n",
    "- Measures angle between vectors\n",
    "- Range: -1 (opposite) to 1 (identical)\n",
    "- 0 = unrelated, 1 = same meaning\n",
    "\n",
    "**Why this matters:**\n",
    "- Retrieval is based on finding the most similar vectors\n",
    "- Quality of retrieval depends on quality of similarity measurement\n",
    "\n",
    "**Why this matters for RAG:**\n",
    "- When you ask a question, your query is converted to a vector\n",
    "- The system finds document chunks with the smallest angles to your query vector\n",
    "- These closest matches are retrieved and used to generate your answer\n",
    "- Better similarity measurement = More relevant context = Better answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Top-k Retrieval\n",
    "\n",
    "**What is top-k?**\n",
    "- k = number of chunks to retrieve\n",
    "- Retrieve the k most similar chunks to the query\n",
    "\n",
    "**Choosing k:**\n",
    "- Too small (k=1): May miss relevant information\n",
    "- Too large (k=50): Too much noise, exceeds context window\n",
    "- Typical values: k=3 to 10\n",
    "- Trade-off: recall (get all relevant info) vs precision (avoid noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Context Window\n",
    "\n",
    "**What is a context window?**\n",
    "- Maximum amount of text an LLM can process at once\n",
    "- Measured in tokens (roughly 0.75 words per token)\n",
    "\n",
    "**Why this matters for RAG:**\n",
    "- Limits how many chunks you can include\n",
    "- Must fit: system message + retrieved chunks + query + room for answer\n",
    "- Example: 4K context window â†’ maybe 2K for retrieved context\n",
    "\n",
    "**Modern LLMs:**\n",
    "- GPT-3.5: 4K or 16K tokens\n",
    "- GPT-4: 8K, 32K, or 128K tokens\n",
    "- Claude: 200K tokens\n",
    "- Gemini: 1M+ tokens\n",
    "\n",
    "**Note:** Longer context â‰  always better for RAG. Quality of retrieval matters more than quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Benefits of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Up-to-Date Information\n",
    "Update knowledge base without retraining models, access real-time data, and reflect latest changes immediately.\n",
    "\n",
    "## - Reduced Hallucinations\n",
    "Ground responses in factual documents where the LLM refers to retrieved context rather than just memory, and can verify claims against sources.\n",
    "\n",
    "## - Source Attribution\n",
    "Cite specific documents and page numbers so users can verify information, building trust and accountability which is critical for legal, medical, and financial use cases.\n",
    "\n",
    "## - Domain Specialization\n",
    "Access specialized knowledge bases without needing to fine-tune on domain data, and swap knowledge bases for different use cases.\n",
    "\n",
    "## - Cost-Effective Updates\n",
    "Updating knowledge base is cheap and fast, while fine-tuning is expensive and slow, and retraining from scratch is prohibitively expensive.\n",
    "\n",
    "## - Privacy and Security\n",
    "Keep proprietary data in your own systems without sending sensitive data for model training, control access to documents, and audit what information was used.\n",
    "\n",
    "## - Transparency\n",
    "Show users what documents were used, explain where answers came from, and build trust through transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Limitations and Challenges of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Retrieval Quality\n",
    "If retrieval fails, answer will be poor following the \"garbage in, garbage out\" principle. This depends heavily on chunk quality and similarity metrics.\n",
    "\n",
    "## - Complexity\n",
    "More moving parts than standalone LLM. Need to manage chunking, embeddings, vector DB, and retrieval logic, which means more things that can break.\n",
    "\n",
    "## - Latency\n",
    "Retrieval adds time to response. Embedding query plus similarity search plus LLM generation typically takes 1-3 seconds versus 0.5-1 second for direct LLM.\n",
    "\n",
    "## - Cost\n",
    "Embedding model costs (if using API), vector database costs (if managed service), and LLM generation costs make it higher than standalone LLM.\n",
    "\n",
    "## - Context Window Limitations\n",
    "Can only retrieve limited information. May not fit all relevant context, so you need to choose what to include.\n",
    "\n",
    "## - Chunking Challenges\n",
    "How to chunk documents optimally involves trade-offs between chunk size and precision, and important context may be split across chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. When NOT to Use RAG\n",
    "\n",
    "RAG is not always the right solution. Don't use RAG when:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - No External Knowledge Needed\n",
    "Task is pure reasoning or instruction-following. Example: \"Translate this text to French\" requires no retrieval.\n",
    "\n",
    "## - Knowledge Fits in Prompt\n",
    "Small, static knowledge base where you can include all necessary context in prompt directly. Example: Company with 10 simple FAQs.\n",
    "\n",
    "## - Need Behavior Change, Not Knowledge\n",
    "Want to change writing style or tone, or need to follow specific output formats. Fine-tuning is better for behavior modification.\n",
    "\n",
    "## - Extremely Low Latency Required\n",
    "Real-time applications where every millisecond counts and retrieval overhead is unacceptable. Consider caching\n",
    "\n",
    "## - Privacy Constraints Prevent External Retrieval\n",
    "Can't connect to external systems, completely offline environment, or no way to index documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLMs have limitations:** knowledge cutoff, hallucinations, lack of private data access\n",
    "\n",
    "2. **RAG addresses these** by retrieving relevant information and including it in the prompt\n",
    "\n",
    "3. **RAG Pipeline:** Index documents â†’ Embed query â†’ Retrieve similar chunks â†’ Generate answer\n",
    "\n",
    "4. **Use RAG when:** Need external knowledge, information changes, want source citations\n",
    "\n",
    "5. **Don't use RAG when:** No external knowledge needed, knowledge fits in prompt, need behavior change\n",
    "\n",
    "6. **Trade-offs:** RAG adds complexity and latency but provides accuracy, citations, and updatability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "In **Module 2: Document Processing & Chunking**, you'll learn:\n",
    "- How to split documents effectively\n",
    "- Different chunking strategies and when to use them\n",
    "- How to implement chunking from scratch\n",
    "- How to preserve metadata\n",
    "\n",
    "Get ready to get hands-on!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
