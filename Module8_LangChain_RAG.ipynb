{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: RAG with LangChain\n",
    "\n",
    "## Overview\n",
    "\n",
    "Build complete RAG systems using LangChain's modern LCEL (LangChain Expression Language).\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Use LangChain core components (loaders, splitters, embeddings, vector stores)\n",
    "2. Build RAG chains with LCEL\n",
    "3. Add basic conversational memory\n",
    "4. Save and load vector stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Required packages:**\n",
    "```bash\n",
    "pip install langchain langchain-core langchain-community langchain-openai langchain-text-splitters\n",
    "pip install faiss-cpu python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"‚úÖ API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LangChain Core Components\n",
    "\n",
    "### 1.1 Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 documents\n",
      "First document: LangChain is a framework for developing applications powered by language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create documents directly (you can also load from files using loaders)\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
    "    Document(page_content=\"RAG combines retrieval with generation to provide accurate, grounded responses.\"),\n",
    "    Document(page_content=\"Vector stores enable efficient similarity search over embedded documents.\"),\n",
    "    Document(page_content=\"FAISS is a library for efficient similarity search developed by Meta AI.\"),\n",
    "    Document(page_content=\"Text splitters chunk documents into smaller pieces for better retrieval.\")\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "print(f\"First document: {documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 5 documents into 5 chunks\n",
      "\n",
      "Chunk 1: LangChain is a framework for developing applications powered by language models.\n",
      "\n",
      "Chunk 2: RAG combines retrieval with generation to provide accurate, grounded responses.\n",
      "\n",
      "Chunk 3: Vector stores enable efficient similarity search over embedded documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1536\n",
      "First 5 values: [0.0006903278990648687, 0.025743499398231506, 0.007137471344321966, 0.03336307406425476, -0.03193636238574982]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"What is RAG?\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created with 5 chunks\n",
      "\n",
      "Query: What is FAISS?\n",
      "\n",
      "Result 1: FAISS is a library for efficient similarity search developed by Meta AI.\n",
      "\n",
      "Result 2: RAG combines retrieval with generation to provide accurate, grounded responses.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {len(chunks)} chunks\")\n",
    "\n",
    "# Test similarity search\n",
    "query = \"What is FAISS?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building RAG with LCEL (LangChain Expression Language)\n",
    "\n",
    "### 2.1 Simple RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain created\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer using ONLY the provided context.\"),\n",
    "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    RunnableParallel(context=retriever | format_docs, question=RunnablePassthrough())\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by language models.\n"
     ]
    }
   ],
   "source": [
    "# Query the chain\n",
    "response = rag_chain.invoke(\"What is LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector search is a type of search that involves finding similar items based on their vector representations in a high-dimensional space. It is commonly used in applications such as recommendation systems, image search, and document retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Create a custom prompt with specific instructions\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a precise assistant. If you don't know the answer based on the context, say 'I don't know'.\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Build chain with custom prompt\n",
    "custom_rag = (\n",
    "    RunnableParallel(context=retriever | format_docs, question=RunnablePassthrough())\n",
    "    | custom_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Query\n",
    "response = custom_rag.invoke(\"What is vector search?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Simple Conversational RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversational chain created\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Store for chat histories\n",
    "chat_store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in chat_store:\n",
    "        chat_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_store[session_id]\n",
    "\n",
    "# Create conversational prompt\n",
    "conv_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer using the context provided.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Build base chain\n",
    "conv_chain_base = (\n",
    "    RunnableParallel(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        question=lambda x: x[\"question\"],\n",
    "        chat_history=lambda x: x.get(\"chat_history\", [])\n",
    "    )\n",
    "    | conv_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with message history\n",
    "conv_chain = RunnableWithMessageHistory(\n",
    "    conv_chain_base,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is FAISS?\n",
      "A1: FAISS is a library developed by Meta AI for efficient similarity search, which can be used in combination with RAG to enhance retrieval capabilities for generating accurate responses.\n",
      "\n",
      "Q2: Who developed it?\n",
      "A2: FAISS was developed by Meta AI.\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "response1 = conv_chain.invoke(\n",
    "    {\"question\": \"What is FAISS?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
    ")\n",
    "print(f\"Q1: What is FAISS?\")\n",
    "print(f\"A1: {response1}\\n\")\n",
    "\n",
    "# Follow-up question (remembers context)\n",
    "response2 = conv_chain.invoke(\n",
    "    {\"question\": \"Who developed it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
    ")\n",
    "print(f\"Q2: Who developed it?\")\n",
    "print(f\"A2: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat History:\n",
      "\n",
      "human: What is FAISS?\n",
      "\n",
      "ai: FAISS is a library developed by Meta AI for efficient similarity search.\n",
      "\n",
      "human: What is FAISS?\n",
      "\n",
      "ai: FAISS is a library developed by Meta AI for efficient similarity search, which can be used in combination with RAG to enhance retrieval capabilities for generating accurate responses.\n",
      "\n",
      "human: Who developed it?\n",
      "\n",
      "ai: FAISS was developed by Meta AI.\n"
     ]
    }
   ],
   "source": [
    "# View chat history\n",
    "session = get_session_history(\"session1\")\n",
    "print(\"Chat History:\")\n",
    "for msg in session.messages:\n",
    "    print(f\"\\n{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is FAISS?\n",
      "A1: FAISS is a library developed by Meta AI for efficient similarity search.\n",
      "\n",
      "Q2: Who developed it?\n",
      "A2: FAISS was developed by Meta AI.\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "response1 = conv_chain.invoke(\n",
    "    {\"question\": \"What is FAISS?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session3\"}}\n",
    ")\n",
    "print(f\"Q1: What is FAISS?\")\n",
    "print(f\"A1: {response1}\\n\")\n",
    "\n",
    "# Follow-up question (remembers context)\n",
    "response2 = conv_chain.invoke(\n",
    "    {\"question\": \"Who developed it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session4\"}}\n",
    ")\n",
    "print(f\"Q2: Who developed it?\")\n",
    "print(f\"A2: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: The Memory Mystery\n",
    "**Your Task:**\n",
    "\n",
    "1. **Run the code above** - What happens? Does it work?\n",
    "2. **Question:** If `session4` has NO memory of `session1`, why does        \n",
    "\"Who developed it?\" give a correct answer about FAISS?\n",
    "3. **Investigate:** What is REALLY providing the context for the answer?    \n",
    "(Hint: It's not memory!)\n",
    "4. **Prove it:** Design a simple test to confirm your hypothesis.\n",
    "\n",
    "**Bonus Challenge:** What would happen if you asked \"Who developed it?\"     \n",
    "in a brand new session5 WITHOUT asking about FAISS first?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Saving and Loading Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vector store\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"‚úÖ Vector store saved\")\n",
    "\n",
    "# Load vector store\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "print(\"‚úÖ Vector store loaded\")\n",
    "\n",
    "# Test\n",
    "test_results = loaded_vectorstore.similarity_search(\"LangChain\", k=1)\n",
    "print(f\"\\nTest result: {test_results[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "1. **Core Components:**\n",
    "   - Documents and loaders\n",
    "   - Text splitters for chunking\n",
    "   - OpenAI embeddings\n",
    "   - FAISS vector store\n",
    "\n",
    "2. **Modern RAG with LCEL:**\n",
    "   - Simple RAG chains using `|` pipe operator\n",
    "   - Returning source documents\n",
    "   - Custom prompts\n",
    "   - Streaming responses\n",
    "\n",
    "3. **Conversational RAG:**\n",
    "   - Adding message history\n",
    "   - Session management\n",
    "\n",
    "4. **Persistence:**\n",
    "   - Saving and loading vector stores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources for Self-Study\n",
    "\n",
    "**Official Documentation:**\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LCEL Conceptual Guide](https://python.langchain.com/docs/expression_language/)\n",
    "- [Retrieval Conceptual Guide](https://python.langchain.com/docs/modules/data_connection/)\n",
    "\n",
    "**Key Topics to Explore:**\n",
    "1. **Document Loaders** - Try loading PDFs, web pages, and structured data\n",
    "2. **Advanced Text Splitting** - Explore different splitters and optimal chunking strategies\n",
    "3. **Alternative Vector Stores** - Compare Chroma vs FAISS\n",
    "4. **Advanced Retrieval Strategies** - Implement MMR, multi-query retrieval, and contextual compression\n",
    "5. **Prompt Engineering for RAG** - Techniques to reduce hallucination and improve accuracy\n",
    "6. **Advanced Conversational RAG** - Query reformulation and context window management\n",
    "7. **Metadata Filtering** - Add and filter documents by metadata for better retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ RAG Project 1: Build Your RAG Assistant\n",
    "\n",
    "### Project Overview\n",
    "Build a complete RAG (Retrieval-Augmented Generation) system using LangChain that answers questions about documents relevant to YOU. This is a portfolio project - make it meaningful and showcase-worthy!\n",
    "\n",
    "**Why This Matters:**\n",
    "- Apply everything you've learned in a real-world scenario\n",
    "- Create something you'll actually use\n",
    "- Build a portfolio piece that demonstrates your AI skills\n",
    "- Lay the foundation for more complex RAG systems\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Project Requirements\n",
    "\n",
    "#### **What You Must Build:**\n",
    "\n",
    "1. **Your Document Collection**\n",
    "   - Choose documents that matter to YOU \n",
    "   - Load and process at least 5-10 documents\n",
    "   - Experiment with different chunk sizes and overlaps\n",
    "\n",
    "2. **Vector Store with ChromaDB**\n",
    "   - Create and persist embeddings\n",
    "   - Ensure your data is saved for future sessions\n",
    "\n",
    "3. **RAG Chain with LCEL**\n",
    "   - Build using LangChain Expression Language\n",
    "   - Custom prompt tailored to your use case\n",
    "   - Return answers with source citations\n",
    "\n",
    "4. **Conversational Memory**\n",
    "   - Add chat history functionality\n",
    "   - Handle follow-up questions naturally\n",
    "   - Maintain context across questions\n",
    "\n",
    "5. **Documentation & Testing**\n",
    "   - Test with real questions you'd ask your system\n",
    "   - Document your process and learnings\n",
    "   - Write clear instructions for running your project\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Make It Portfolio-Worthy\n",
    "\n",
    "#### **Project Structure**\n",
    "```\n",
    "my-rag-project/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ documents/              # Your documents\n",
    "‚îú‚îÄ‚îÄ chroma_db/             # Vector store\n",
    "‚îú‚îÄ‚îÄ rag_system.ipynb       # Main notebook\n",
    "‚îú‚îÄ‚îÄ .env                   # API keys\n",
    "‚îú‚îÄ‚îÄ requirements.txt       \n",
    "‚îî‚îÄ‚îÄ README.md             # Tell your story\n",
    "```\n",
    "\n",
    "### ‚úÖ What to Submit\n",
    "\n",
    "**Your Notebook Should Include:**\n",
    "- Complete working code\n",
    "- Comments explaining your decisions\n",
    "- Test questions and responses\n",
    "- Observations from your experiments\n",
    "\n",
    "**Your README Should Tell:**\n",
    "- What problem you're solving\n",
    "- What documents you're using (and why)\n",
    "- How to run your system\n",
    "- Example interactions\n",
    "- What you learned\n",
    "- Future improvements you'd make\n",
    "\n",
    "**Include:**\n",
    "- requirements.txt\n",
    "- Your saved ChromaDB\n",
    "- Clear instructions\n",
    "\n",
    "### ‚è∞ Deadline\n",
    "**Sunday, December 15, 2025 at 11:59 PM**\n",
    "Submit your or GitHub repository link before the deadline. Submission link will be provided.\n",
    "\n",
    "\n",
    "**Good luck!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
