{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Introduction to Orchestration Frameworks\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Modules 1-6, you built RAG systems from scratch to understand every component deeply. Now we'll explore **orchestration frameworks** that simplify RAG development.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand why orchestration frameworks exist\n",
    "2. Compare LangChain and LlamaIndex\n",
    "3. Learn when to use frameworks vs custom code\n",
    "4. Explore core framework components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Use Orchestration Frameworks?\n",
    "\n",
    "### The Problem with Custom Implementations\n",
    "\n",
    "When you built RAG from scratch (Module 6), you wrote code for:\n",
    "- Document loading (PDF, TXT, etc.)\n",
    "- Text chunking strategies\n",
    "- Embedding generation\n",
    "- Vector store integration\n",
    "- Retrieval logic\n",
    "- Prompt construction\n",
    "- LLM API calls\n",
    "\n",
    "**This is valuable for learning, but time-consuming for production.**\n",
    "\n",
    "### What Frameworks Provide\n",
    "\n",
    "1. **Abstraction**: Pre-built components for common tasks\n",
    "2. **Integrations**: Out-of-the-box support for 100+ tools (LLMs, vector stores, loaders)\n",
    "3. **Best Practices**: Battle-tested implementations\n",
    "4. **Community**: Shared knowledge, examples, and support\n",
    "5. **Speed**: Prototype RAG systems in minutes instead of hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Framework Comparison: LangChain vs LlamaIndex\n",
    "\n",
    "### LangChain\n",
    "\n",
    "**Philosophy:** Modular components for building LLM-powered applications\n",
    "\n",
    "**Strengths:**\n",
    "- Rich ecosystem (agents, tools, memory)\n",
    "- Highly flexible chain and workflow composition\n",
    "- Excellent for multi-step reasoning and tool use\n",
    "- Strong community and integrations\n",
    "\n",
    "**Use Cases:**\n",
    "- Agent-driven applications\n",
    "- Complex multi-step LLM workflows\n",
    "- Chatbots with state and memory\n",
    "- Systems requiring external tool/API integration\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Chains**: Structured sequences of LLM operations\n",
    "- **Retrievers**: Fetch relevant documents\n",
    "- **Document Loaders**: Load data from sources\n",
    "- **Text Splitters**: Chunk documents\n",
    "\n",
    "---\n",
    "\n",
    "### LlamaIndex\n",
    "\n",
    "**Philosophy:** Data framework for connecting LLMs to your data\n",
    "\n",
    "**Strengths:**\n",
    "- Advanced retrieval and indexing options\n",
    "- Clean, intuitive API design\n",
    "- Excellent handling of both structured and unstructured data\n",
    "- Powerful query engines and RAG optimizations\n",
    "\n",
    "**Use Cases:**\n",
    "- Document question-answering\n",
    "- Semantic and hybrid search\n",
    "- Data-centric applications\n",
    "- Quick RAG prototypes\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Indices**: Organized data structures (VectorStoreIndex, TreeIndex)\n",
    "- **Query Engines**: Execute queries across indices\n",
    "- **Readers**: Load data from sources\n",
    "- **Node Parsers**: Chunk and structure documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | LangChain | LlamaIndex |\n",
    "|---------|-----------|------------|\n",
    "| **Primary Focus** | LLM orchestration (agents, tools, workflows) | Data indexing, retrieval, and RAG optimization |\n",
    "| **Design Philosophy** | Modular, flexible, component-driven | Data-centric, simple, retrieval-first |\n",
    "| **Best For** | Agents, tool-using systems, complex pipelines | Question-answering, semantic search, pure RAG |\n",
    "| **Learning Curve** | Medium‚ÄìHigh | Low‚ÄìMedium |\n",
    "| **Community Size** | Very large | Fast-growing |\n",
    "| **GitHub Stars** | 90k+ | 30k+ |\n",
    "| **Documentation** | Extensive but sometimes scattered | Clean, focused, very practical |\n",
    "| **API Complexity** | Verbose and highly customizable | Concise and easy to prototype |\n",
    "| **Integrations** | 100+ tools, models, services | Fewer integrations but deep RAG features |\n",
    "| **Version Stability** | Frequent updates & breaking changes | Generally more stable across versions |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Comparison Example\n",
    "\n",
    "Let's see the same RAG task in both frameworks and compare to custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not found. Set it in .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for all examples\n",
    "documents = [\n",
    "    \"Python is a high-level programming language known for readability and simplicity.\",\n",
    "    \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "    \"RAG combines retrieval and generation to provide accurate, grounded responses.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Custom Implementation (From Module 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RAG (simplified from Module 6)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "doc_embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Query and retrieve\n",
    "query = \"What is RAG?\"\n",
    "query_embedding = model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute similarity\n",
    "similarities = np.dot(doc_embeddings, query_embedding)\n",
    "top_idx = np.argmax(similarities)\n",
    "retrieved_doc = documents[top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate response\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "prompt = f\"\"\"Context: {retrieved_doc}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Custom RAG Answer:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RAG (simplified from Module 6)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "# 2. Query and retrieve\n",
    "query = \"What is RAG?\"\n",
    "query_embedding = model.encode([query])[0]\n",
    "\n",
    "# 3. Compute similarity\n",
    "similarities = np.dot(doc_embeddings, query_embedding)\n",
    "top_idx = np.argmax(similarities)\n",
    "retrieved_doc = documents[top_idx]\n",
    "\n",
    "# 4. Generate response\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "prompt = f\"\"\"Context: {retrieved_doc}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Custom RAG Answer:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lines of code:** ~20 lines\n",
    "\n",
    "**Pros:**\n",
    "- Full control over every step\n",
    "- No external dependencies (except libraries)\n",
    "- Easy to debug\n",
    "\n",
    "**Cons:**\n",
    "- Manual handling of embeddings, similarity, prompts\n",
    "- Need to write boilerplate for each component\n",
    "- Scaling requires more custom code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: LangChain Implementation with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install langchain langchain-core langchain-community langchain-openai langchain-text-splitters\n",
    "pip install faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Convert documents\n",
    "lc_docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "# Vector store\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore = FAISS.from_documents(lc_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert assistant. Use ONLY the retrieved context.\"),\n",
    "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "# Build RAG pipeline\n",
    "rag_chain = (\n",
    "    RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Query\n",
    "response = rag_chain.invoke(\"What is RAG?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: LangChain Implementation with Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Convert documents\n",
    "lc_docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "# Vector store (Chroma)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# persist_directory allows saving DB locally; optional\n",
    "vectorstore = Chroma.from_documents(\n",
    "    lc_docs,\n",
    "    embeddings,\n",
    "    collection_name=\"my_rag_collection\",\n",
    "    persist_directory=\"./chroma_db\"  # optional\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert assistant. Use ONLY the retrieved context.\"),\n",
    "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "# Build RAG pipeline\n",
    "rag_chain = (\n",
    "    RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Query\n",
    "response = rag_chain.invoke(\"What is RAG?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lines of code:** ~10 lines\n",
    "\n",
    "**Pros:**\n",
    "- Abstracted embedding and retrieval\n",
    "- Built-in chain composition\n",
    "- Easy to swap components (different LLMs, vector stores)\n",
    "\n",
    "**Cons:**\n",
    "- Less visibility into internal steps\n",
    "- Framework dependency\n",
    "- Learning curve for LangChain concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: LlamaIndex Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex, Settings\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Configure LlamaIndex\n",
    "Settings.llm = LlamaOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=openai_api_key)\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=openai_api_key)\n",
    "\n",
    "# Create documents and index\n",
    "llama_docs = [Document(text=doc) for doc in documents]\n",
    "index = VectorStoreIndex.from_documents(llama_docs)\n",
    "\n",
    "# Query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is RAG?\")\n",
    "\n",
    "print(\"LlamaIndex Answer:\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lines of code:** ~7 lines\n",
    "\n",
    "**Pros:**\n",
    "- Simplest API (most concise)\n",
    "- Optimized for data indexing\n",
    "- Intuitive for RAG-specific tasks\n",
    "\n",
    "**Cons:**\n",
    "- Less flexible for non-RAG workflows\n",
    "- Smaller ecosystem compared to LangChain\n",
    "- Framework dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. When to Use What?\n",
    "\n",
    "### Use Custom Code When:\n",
    "- Learning RAG fundamentals (you already did this!)\n",
    "- Simple, specific use case with unique requirements\n",
    "- You need full control over every component\n",
    "- Minimizing dependencies is critical\n",
    "- Performance optimization requires custom logic\n",
    "\n",
    "### Use LangChain When:\n",
    "- Building complex multi-step workflows\n",
    "- Creating agents that use multiple tools\n",
    "- Need conversational memory\n",
    "- Integrating many external services\n",
    "- Prototyping chatbots or assistants\n",
    "\n",
    "### Use LlamaIndex When:\n",
    "- Primary goal is question-answering over documents\n",
    "- Want the simplest API for RAG\n",
    "- Need advanced indexing strategies (tree, graph)\n",
    "- Working with structured + unstructured data\n",
    "- Quick prototypes for data retrieval\n",
    "\n",
    "### Hybrid Approach:\n",
    "You can mix custom code with frameworks:\n",
    "- Use LangChain for retrieval, custom code for generation\n",
    "- Use LlamaIndex for indexing, custom logic for post-processing\n",
    "- Build custom components within framework pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trade-offs Summary\n",
    "\n",
    "| Aspect | Custom Code | LangChain | LlamaIndex |\n",
    "|--------|-------------|-----------|------------|\n",
    "| **Learning Curve** | Low (Python basics) | Medium-High | Low-Medium |\n",
    "| **Development Speed** | Slow | Fast | Very Fast |\n",
    "| **Flexibility** | Highest | High | Medium |\n",
    "| **Code Length** | Longest | Medium | Shortest |\n",
    "| **Dependencies** | Minimal | Many | Moderate |\n",
    "| **Community Support** | N/A | Large | Growing |\n",
    "| **Best For** | Learning, custom needs | Complex workflows | RAG-focused apps |\n",
    "| **Maintenance** | You own it | Follow updates | Follow updates |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "1. **Frameworks accelerate development** but add dependencies\n",
    "2. **LangChain** is broader (agents, chains, tools) with a larger ecosystem\n",
    "3. **LlamaIndex** is specialized for data retrieval with simpler API\n",
    "4. **Custom code** is still valuable for learning and specific requirements\n",
    "5. **You can mix approaches** - use frameworks where helpful, custom code where needed\n",
    "\n",
    "In the next modules:\n",
    "- **Module 9:** Deep dive into LangChain RAG\n",
    "- **Module 10:** Deep dive into LlamaIndex RAG\n",
    "- **Module 11:** Production-ready RAG systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Practice Exercises\n",
    "\n",
    "## Exercise 1: Personal Project Planning\n",
    "\n",
    "### Task 1\n",
    "Think of a RAG project you'd like to build (or one from your Module 6/7 work).\n",
    "\n",
    "**Answer these questions:**\n",
    "\n",
    "1. **Project Description:**\n",
    "   ___________\n",
    "\n",
    "2. **Would you use a framework or custom code? Why?**\n",
    "   ___________\n",
    "\n",
    "3. **If using a framework, which one and why?**\n",
    "   ___________\n",
    "\n",
    "4. **What components would you keep custom (if any)?**\n",
    "   ___________\n",
    "\n",
    "5. **What are the main risks of your choice?**\n",
    "   ___________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "publica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
